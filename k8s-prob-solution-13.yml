ğ‘«ğ’†ğ’ƒğ’–ğ’ˆğ’ˆğ’Šğ’ğ’ˆ ğ‘²ğ’–ğ’ƒğ’†ğ’“ğ’ğ’†ğ’•ğ’†ğ’” ğ‘·ğ’ğ’… ğ’‡ğ’‚ğ’Šğ’ğ’–ğ’“ğ’†ğ’” â˜¸ ğŸ¤”

ğ˜—ğ˜°ğ˜¥ğ˜´ ğ˜¤ğ˜¢ğ˜¯ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ’”ğ’•ğ’‚ğ’“ğ’•ğ’–ğ’‘ ğ˜¢ğ˜¯ğ˜¥ ğ’“ğ’–ğ’ğ’•ğ’Šğ’ğ’† ğ˜¦ğ˜³ğ˜³ğ˜°ğ˜³ğ˜´ 
ğŸ“Œ ğ‘ºğ’•ğ’‚ğ’“ğ’•ğ’–ğ’‘ ğ’†ğ’“ğ’“ğ’ğ’“ğ’” ğ’Šğ’ğ’„ğ’ğ’–ğ’…ğ’†:
âœ… ImagePullBackoff
âœ… ImageInspectError
âœ… ErrImagePull
âœ… ErrImageNeverPull
âœ… RegistryUnavailable
âœ… InvalidImageName

ğŸ“Œ ğ‘¹ğ’–ğ’ğ’•ğ’Šğ’ğ’† ğ’†ğ’“ğ’“ğ’ğ’“ğ’” ğ’Šğ’ğ’„ğ’ğ’–ğ’…ğ’†:
âœ… CrashLoopBackOff
âœ… RunContainerError
âœ… KillContainerError
âœ… VerifyNonRootError
âœ… RunInitContainerError
âœ… CreatePodSandboxError
âœ… ConfigPodSandboxError
âœ… KillPodSandboxError
âœ… SetupNetworkError
âœ… TeardownNetworkError

â—ğ‘°ğ’ğ’‚ğ’ˆğ’†ğ‘·ğ’–ğ’ğ’ğ‘©ğ’‚ğ’„ğ’Œğ‘¶ğ’‡ğ’‡
âœ This error appears when hashtag#k8s isn't able to retrieve the image for one of the hashtag#containers of the Pod.
There are three common culprits:
âœ… The image name is invalid
âœ… You specified a non-existing tag for the image.
âœ… The image that you're trying to retrieve belongs to a private registry and the cluster doesn't have credentials to access it.
The first two cases can be solved by correcting the image name and tag.
For the last, one should add the credentials to your private registry in a Secret and reference it in the Pods

â—ğ‘¹ğ’–ğ’ğ‘ªğ’ğ’ğ’•ğ’‚ğ’Šğ’ğ’†ğ’“ğ‘¬ğ’“ğ’“ğ’ğ’“
âœ The error appears when the container is unable to start before application
Common causes:
âœ… Mounting a not-existent volume such as ConfigMap or Secrets
âœ… Mounting a read-only volume as read-write
More detailed aspect can be found by describing the 'failed' pod

â—ğ‘ªğ’“ğ’‚ğ’”ğ’‰ğ‘³ğ’ğ’ğ’‘ğ‘©ğ’‚ğ’„ğ’Œğ‘¶ğ’‡ğ’‡
âœ If the container can't start, then hashtag#Kubernetes shows the CrashLoopBackOff message as a status.
Usually, a container can't start when:
âœ… There's an error in the application that prevents it from starting.
âœ… You misconfigured the container.
âœ… The Liveness probe failed too many times.

â—ğ‘·ğ’ğ’…ğ’” ğ’Šğ’ ğ’‚ ğ‘·ğ’†ğ’ğ’…ğ’Šğ’ğ’ˆ ğ’”ğ’•ğ’‚ğ’•ğ’†
âœ Assuming that the scheduler component is running fine, here are the causes:
âœ… The cluster doesn't have enough resources such as CPU and memory to run the Pod.
âœ… The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota.
âœ… The Pod is bound to a Pending PersistentVolumeClaim.
The best option is to inspect the Events section in the "kubectl describe"

PoV - It's never easy to reach a successful deployment without few


ğ‡ğ¨ğ° ğğ¨ğğ¬ ğš hashtag#ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğğ¨ğ ğ¢ğ¬ ğšğ¬ğ¬ğ¢ğ ğ§ğğ ğšğ§ ğˆğ ğšğğğ«ğğ¬ğ¬ â“

Setting up hashtag#Networking on a hashtag#k8s cluster is essentially the interaction between ğ‘²ğ’–ğ’ƒğ’†ğ’ğ’†ğ’• <=> ğ‘ªğ‘µğ‘° (Container Networking Interface) 
<=> ğ‘ªğ‘¹ğ‘° (Container Runtime Interface) ğŸš€ 

ğŸ‘‰ Kube-controller-manager assigns a podCIDR to each node in the cluster
ğŸ‘‰ Pods on a node are assigned an IP address from the subnet value in podCIDR.
ğŸ‘‰ Because podCIDRs across all nodes are disjoint subnets, it allows assigning each pod a unique IP address.
ğŸ‘‰ The k8s cluster administrator configures and installs kubelet,
hashtag#container runtime, network provider agent and distributes hashtag#CNI plugins on each node.
ğŸ‘‰ When a network provider agent starts, it generates a CNI config.
ğŸ‘‰ When a pod is scheduled on a node, kubelet calls the hashtag#CRI plugin to create the pod on the node assigned
ğŸ‘‰ The CNI plugin specified in the CNI config configures the pod network resulting in a pod getting an IP address !!


ğ–ğ¡ğšğ­ ğ¢ğŸ ğ­ğ¡ğ ğ©ğ¨ğ ğ¥ğ¢ğ¦ğ¢ğ­ ğœğ«ğ¨ğ¬ğ¬ğğ¬ 110 ? ğŸ¤” 

In a Kubernetes cluster, the default allowed maximum is 110 pods per node. Why that so & what happens if we try to increase? 
Is there a way to override the default value â—â“ 

ğŸ‘‰ In hashtag#k8s, each node is assigned a range of IP Addresses, a CIDR Block and thus each pod gets its unique IP address
ğŸ‘‰ Kubernetes assigns a /24 CIDR block (i.e. 256 addresses) for each node to capacitate 110 pods requirement
ğŸ‘‰ While the general recommendation is to have an approximation of twice as many IP addresses as the available number of pods in the node. 
This is to address a seamless IP address mitigation when a pod gets added/removed. 110 Pods and 256 IPS. Sounds perfect, right?

Well, this is not a hard limit and one can extend the pod limit manually to the extent of 256 under default CIDR /24 considering the fact, 
you are in a tight situation there is one IP address per one pod. 
However, it might lead to container creation error as the pod many not be able to start up in certain cases.

ğ‘¯ğ’ğ’˜ ğ’•ğ’ ğ’Šğ’ğ’„ğ’“ğ’†ğ’‚ğ’”ğ’† ğ’•ğ’‰ğ’† ğ’…ğ’†ğ’‡ğ’‚ğ’–ğ’ğ’• ğ’‘ğ’ğ’… ğ’ğ’Šğ’ğ’Šğ’•?

It is possible to bypass the required pod limit by passing it to the field max-pods in the Kubernetes configuration file.

$KUBELET_EXTRA_ARGS â€” max-pods=240



ğ‘¯ğ’ğ’˜ ğ’•ğ’ ğ’“ğ’†ğ’‘ğ’ğ’‚ğ’„ğ’† ğ’Šğ’‘ğ’•ğ’‚ğ’ƒğ’ğ’†ğ’” ğ’Šğ’ 'ğ’Œğ’–ğ’ƒğ’†-ğ’‘ğ’“ğ’ğ’™ğ’š' & ğ’ƒğ’“ğ’Šğ’ğ’ˆ ğ’†ğ‘©ğ‘·ğ‘­ ğ’Šğ’ğ’•ğ’ ğ‘²ğ’–ğ’ƒğ’†ğ’“ğ’ğ’†ğ’•ğ’†ğ’”? ğŸ¤” 

kube-proxy as a L3/L4 network proxy maintains networking-rules together with load-balancer functionality for facilitating 
communication between services and pods. OOTB, it leverages 'iptables' for packet forwarding, load balancing, and service 
abstraction in hashtag#Kubernetes
IPtables are based on:
â¡ iptables table ~ a way to group together chains of rules
â¡ iptables chain ~ ordered list of rules that is evaluated sequentially when a packet traverses the chain

However, iptables has its own caveats and short-falls
â— Increase latency and poor performance while managing large hashtag#k8s clusters
â— Architectural design puts significant overhead in managing even few clusters with internal re-routing to & fro of multiple packets
â— Debugging iptables is frustrating, with hundreds to thousands of rules to debug and updating iptables rules for any fix
â— Not a scalable solution to manage increase in complexity of k8s env

hashtag#eBPF has been a modernized choice to move beyond iptables for hashtag#linux kernel community owing to 
âœ… Lesser CPU overhead handling large-scale operations, such as load balancing
âœ… Use of efficient hash tables allows eBPF to scale almost limitlessly, reducing latency and overhead
âœ… Improved TCP benchmarking performance even with bare-metal machines !!

Currently, the industry standard for bringing eBPF into Kubernetes (and by extension replacing kube-proxy) is through
hashtag#Cilium (only hashtag#CNCF graduated hashtag#CNI provider)
Major hashtag#cloud providers have migrated away from kube-proxy and use Cilium as their recommended CNI !!


ğ’ğ¢ğ¦ğ©ğ¥ğ¢ğŸğ²ğ¢ğ§ğ  ğƒğšğ­ğšğ›ğšğ¬ğ ğŒğ¢ğ ğ«ğšğ­ğ¢ğ¨ğ§ ğ¨ğ§ ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğ§ğšğ­ğ¢ğ¯ğğ¥ğ² â˜¸ 
-----------------------------------------------------------
Database migration is a crucial aspect of deploying applications on any
hashtag#Kubernetes cluster ensuring hashtag#database schema and data remain synchronised with the applicationâ€™s topology

Four different hashtag#cloudnative approaches of performing hashtag#db migration ~
ğŸ‘‰ ğ‘°ğ’ğ’Šğ’• ğ‘ªğ’ğ’ğ’•ğ’‚ğ’Šğ’ğ’†ğ’“ğ’” ~ Init containers can be utilised to perform migration tasks before the application containers are deployed requiring minimal changes in the deployment yaml file
ğğ«ğ¨ğ¬
âœ… Isolated migration process
âœ… Simplified deployment manifests
ğ‚ğ¨ğ§ğ¬
ğŸ›‘ Limited flexibility
ğŸ›‘ Increased resource consumption

ğŸ‘‰ ğ‚ğ¨ğ§ğ­ğ¢ğ§ğ®ğ¨ğ®ğ¬ ğƒğğ©ğ¥ğ¨ğ²ğ¦ğğ§ğ­ ğğ¢ğ©ğğ¥ğ¢ğ§ğğ¬ ~ Integrate the database migration process into the applicationâ€™s CI/CD pipeline where the migration 
scripts are triggered, taking the connection parameters as ğ˜¦ğ˜¯ğ˜·ğ˜ªğ˜³ğ˜°ğ˜¯ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ˜·ğ˜¢ğ˜³ğ˜ªğ˜¢ğ˜£ğ˜­ğ˜¦ğ˜´
ğğ«ğ¨ğ¬
âœ… Automated and streamlined process
âœ… Version control
ğ‚ğ¨ğ§ğ¬
ğŸ›‘ Complexity
ğŸ›‘ Tight coupling
ğŸ›‘ Credentials for Database gets exposed

ğŸ‘‰ ğ’ğğ©ğšğ«ğšğ­ğ ğ‡ğğ¥ğ¦ ğ‚ğ¡ğšğ«ğ­ ğ°ğ¢ğ­ğ¡ ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğ‰ğ¨ğ› ~ Leverages the package management capabilities of Helm. The chart includes a 
Kubernetes job that runs an image containing the migration script and is deployed on the hashtag
hashtag#k8s cluster from where the database is directly accessible.
ğğ«ğ¨ğ¬
âœ… Modularity and reusability of helm charts
âœ… Configuration flexibility
âœ… Isolated Migration
âœ… No database exposure beyond the cluster
ğ‚ğ¨ğ§ğ¬
ğŸ›‘ Learning curve
ğŸ›‘ Management overhead
ğŸ›‘ Credential exposure

ğŸ‘‰ ğ‚ğ®ğ¬ğ­ğ¨ğ¦-ğƒğğ¯ğğ¥ğ¨ğ©ğğ ğ’ğğ‹ ğ’ğœğ«ğ¢ğ©ğ­ ğ„ğ±ğğœğ®ğ­ğ¨ğ« ~ a custom-developed SQL script executor is packaged as a container image and deployed 
as a Kubernetes job that can connect to a secret store to retrieve the database connection details securely. 
This approach is an extension of the separate helm chart approach but replacing the database command line utility with custom developed one
ğğ«ğ¨ğ¬
âœ… Flexibility and extensibility
âœ… Secure connection handling from secret store
âœ… Version control
ğ‚ğ¨ğ§ğ¬
ğŸ›‘ Development effort
ğŸ›‘ Image management
ğŸ›‘ Scalability of the Kubernetes cluster


