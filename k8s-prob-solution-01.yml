Question 1: In Linux, the Nice command is used to prioritize the execution of processes.
a. Tell me more about it and if given a choice where do you think it will be useful in your existing project ?

 the nice command provides a flexible mechanism for managing process priorities and optimizing 
 system resource utilization in Unix-like operating systems. 
 It helps ensure fair allocation of CPU time and improves overall system responsiveness.


Batch Job Management:

nice is commonly used in batch job scheduling to control the priority of batch processes. 
Batch jobs are typically background tasks that can run when system load is low, and nice 

 nice command in Unix-like operating systems (including Linux) is used to execute a command with 
 modified scheduling priority (niceness).

Suppose you have a CPU-intensive task, such as compressing a large file using the gzip command. 
You want to reduce the impact 
of this task on other processes running on your system by lowering its priority.

nice -n 10 gzip large_file.txt

By running gzip with a higher niceness value, you ensure that it consumes fewer CPU resources and 
has a lower impact on other 
processes running on your system. This helps maintain system responsiveness and ensures that other tasks can run 
smoothly alongside the CPU-intensive compression task

Load average, often referred to simply as "load," is a measure of the average number of processes 
waiting to be executed by the CPU over a certain period of time. 

It's a fundamental metric used in system monitoring and performance analysis on Unix-like operating systems, including Linux

Load average is typically represented as three numbers separated by commas, each corresponding to a different time interval: 
the last 1 minute, the last 5 minutes, and the last 15 minutes.

Load average does not directly measure CPU utilization but rather provides a snapshot of system 
activity and the extent to which the CPU is being utilized

System administrators use load average as one of the key indicators to assess system performance, identify bottlenecks, 
and determine whether additional resources (such as CPU cores or optimizing processes) 
are needed to handle the workload efficiently.
 


b. If you have to prioritize a specific workloads in a dockerd or k8s environment, 
how will you do it similarly to the native Nice command of Linux ?

In Docker and Kubernetes environments, you can prioritize specific workloads by adjusting resource 
allocation and scheduling parameters. 
While Docker and Kubernetes do not have a direct equivalent to the nice command in Linux, you can 
achieve similar effects by utilizing 
resource management features and mechanisms provided by these platforms.

Resource Requests and Limits:

In Kubernetes, you can specify resource requests and limits for CPU and memory for individual 
containers or pods.
By setting higher resource requests for critical workloads, Kubernetes scheduler will give them 
higher priority for resource allocation.


Quality of Service (QoS) Classes:

Kubernetes assigns QoS classes to pods based on their resource requirements and usage.
Pods with higher resource requirements (guaranteed QoS class) are given priority over pods 
with lower resource requirements (burstable or best-effort QoS classes).

Pod Priority and Preemption:

Kubernetes allows you to assign priority classes to pods, indicating their importance relative to other pods.
Pods with higher priority are scheduled and preempt lower-priority pods if necessary to meet resource requirements.


Pod Disruption Budgets (PDBs):

Kubernetes PDBs allow you to specify the minimum number of pods that must remain available 
during disruptions (e.g., node maintenance, pod evictions).
By configuring PDBs for critical workloads, you ensure they are not evicted or disrupted unless necessary.

Node Affinity and Anti-affinity:

Kubernetes allows you to specify node affinity and anti-affinity rules to control pod 
placement based on node labels.
You can use node affinity to ensure critical workloads are scheduled on specific nodes with higher 
resources or better performance characteristics.

Custom Schedulers and Admission Controllers:

You can develop custom schedulers or admission controllers to implement custom scheduling logic 
based on workload priorities or other criteria.
These custom controllers can integrate with Kubernetes API to influence pod scheduling decisions.







Question 2 : What do you know about LINUX Signals, how does Inter-Process Communication happen 
with uses of signals.
Follow-up : How is it related to Kubernetes pod status and to Docker entry points ?



In Linux, signals are a form of inter-process communication (IPC) used for asynchronous 
notifications between processes or between the kernel and processes. Signals are software 
interrupts that notify a process of an event, such as an error condition, termination request, or 
user-defined event. Each signal has a unique identifier represented by an integer.

Here are some key points about Linux signals:

Signal Types: Signals can be generated by the kernel, by another process, or by the 
process itself. Examples of signals include SIGINT (interrupt signal, typically generated by 
pressing Ctrl+C), SIGKILL (termination signal), and SIGUSR1 (user-defined signal).

Signal Handling: Processes can choose how to handle signals they receive. They can:

Ignore the signal.
Let the default action associated with the signal occur (e.g., terminate the process).
Install a signal handler, which is a function that executes when the signal is received. 
This handler can perform custom actions or cleanup before allowing the default action to occur.
Inter-Process Communication (IPC): Signals can be used for basic forms of IPC. 
For example, one process can send a signal to another process to notify it of an event or to request 
termination. However, signals have limitations for IPC, as they are not suited for exchanging large amounts 
of data or for synchronous communication.

Regarding Kubernetes pod status and Docker entry points:

Kubernetes Pod Status: Kubernetes manages containerized applications in units 
called pods. Pods can have different states, such as Running, Pending, or Terminated. 
These states can change due to various factors, including signals sent to the processes 
running within the pods. For example, if a process within a pod fails and terminates unexpectedly,
Kubernetes may detect this and mark the pod as Failed or CrashLoopBackOff.

Docker Entry Points: In Docker, entry point scripts or commands can be specified to 
execute when a container starts. These entry points are often used to initialize the environment, 
set up configurations, and start the main application process. Signals can be relevant 
in this context for managing the lifecycle of the application within the container. 
For instance, a Docker entry point script might install signal handlers to gracefully 
shut down the application when it receives certain signals, allowing for proper cleanup before the container stops.

In summary, Linux signals provide a mechanism for IPC between processes, 
including within containerized environments like Kubernetes pods managed by Docker. 
They can be used for handling events, managing process lifecycles, and 
facilitating communication between processes or between processes and the kernel. 
Signals play a role in managing the status of Kubernetes pods and can be integrated 
into Docker entry points for managing containerized applications.

Question 3 :What are the kubernetes probs that you have used in your deployments, explain their 
importance and pre-requisites for each one.
Follow up:
1. What are the check mechanisms that you used while implementing the probs apart form the httpGet
2. Probs seem very useful, Why doesn't Kubernetes mandate applications to have default probs ?
startup, readiness and liveness all are optional, why ?


In Kubernetes, probes are mechanisms used to determine the health and availability of a 
containerized application running in a pod. There are three types of probes commonly used: 
startup probes, readiness probes, and liveness probes. Each probe serves a 
specific purpose and has its own importance and prerequisites.

Startup Probes:

Importance: Startup probes are used to determine if a container is ready to accept traffic. 
They are executed once after the container starts, and if the probe fails, 
Kubernetes restarts the container. Startup probes are useful for applications that 
require some initialization time before they can handle requests, such as applications that 
need to load large datasets into memory or perform other setup tasks.
Pre-requisites: A startup probe requires the container to expose an endpoint that 
Kubernetes can access to perform the probe. This endpoint should return a success status code 
(2xx or 3xx) if the application is ready to accept traffic.
Readiness Probes:

Importance: Readiness probes are used to determine if a container is ready to serve traffic. 
They are continuously executed, and if the probe fails, Kubernetes stops routing traffic to the container 
until it passes. Readiness probes are crucial for ensuring that only healthy containers receive traffic, 
thus preventing users from accessing unreliable or unstable application instances.
Pre-requisites: Similar to startup probes, readiness probes require the container to expose an 
endpoint for Kubernetes to perform the check. This endpoint should return a success status code 
when the application is ready to serve traffic.
Liveness Probes:

Importance: Liveness probes are used to determine if a container is still running correctly. 
They are executed periodically, and if the probe fails, Kubernetes restarts the container. 
Liveness probes help in detecting and recovering from situations where the application becomes 
unresponsive or deadlocked, ensuring high availability of the application.
Pre-requisites: Like the other probes, liveness probes require the container to expose an 
endpoint for Kubernetes to perform the check. This endpoint should return a success status code if 
the application is still running correctly.
Additional Check Mechanisms:
Apart from httpGet, Kubernetes also supports other types of probes, such as tcpSocket and exec. 
These mechanisms allow Kubernetes to perform checks beyond HTTP endpoints, such as checking if a 
specific TCP port is open or executing a custom command inside the container.

Why Aren't Probes Mandated?:
While probes are incredibly useful for ensuring the reliability and availability of applications in Kubernetes, 
they are not mandated for several reasons:

Flexibility: Different applications have different requirements and characteristics. 
Mandating probes for all applications could limit flexibility and impose unnecessary constraints.
Legacy Systems: Some applications may not have been designed with probe endpoints in mind or 
may not easily support them due to architectural constraints. Mandating probes could make it difficult
to migrate or deploy legacy systems.
Resource Overhead: Continuous probing can introduce additional resource overhead, especially for 
large-scale deployments. For applications where probes are unnecessary or impractical, mandating them 
could lead to unnecessary resource consumption.
Developer Responsibility: Kubernetes follows the principle of letting developers make decisions about 
their applications. While probes are highly recommended for ensuring application health, ultimately 
it's up to the developers to decide whether to implement them based on their specific requirements and constraints.




Question 4 : We spoke about IPC socket or Unix domain socket in Question 2 (find that question in previous posts), 
explain how inter-process communication workflow works in Docker Architecture.
Follow up:
1. What is the role of docker.sock file ?
2. How Docker socket work ?

The Docker daemon exposes a Unix socket (/var/run/docker.sock by default) that acts as an interface 
for communication between 
Docker client commands and the Docker daemon. Here's how the Docker socket works:


Client-Server Architecture:

Docker follows a client-server architecture where the Docker client interacts with the Docker daemon 
(server) to perform various container and image management tasks.
Unix Socket Communication:

The Docker client communicates with the Docker daemon using Unix domain sockets, 
specifically the Docker socket (/var/run/docker.sock).


Daemon Processing:

Upon receiving API requests from the Docker client, the Docker daemon processes the requests and performs
the requested actions.
The Docker daemon manages container lifecycle, image storage, network configuration, volume management, 
and other Docker-related task

Response Transmission:

After completing the requested actions, the Docker daemon sends responses back to the Docker client 
through the Docker socket.
These responses include information about the outcome of the requested operations, such as success messages, 
error messages, and status updates

the Docker socket serves as the communication channel between the Docker client and the Docker daemon, 
enabling users to manage Docker containers and images using Docker client commands.






Question 5 : Design a Kubernetes deployment setup where you have 3 applications that need to run in 
HA to create a solution. 
However the applications have starting and running dependencies and requirements.
a. application 1 should always start first
b. application 2 should always start after application 1 is Ready
c. application 3 should always start after application 2 is Ready
Follow up:
1. How will you design a check mechanism for the applications in such a way that restarts and schedular 
reschedules dont break the dependence requirements
2. How will you plan to incorporate such check mechanism with CD tools where deployments are gitops driven


To design a Kubernetes deployment setup with three applications running in high availability (HA) while ensuring proper sequencing of startup and dependencies, we can use Kubernetes Deployments along with readiness probes and dependencies. Here's how:

Kubernetes Deployment Setup:

Create separate Deployments for each application.
Define readiness probes for each Deployment to ensure that each application is ready to serve traffic before proceeding to the next one.
Dependency Management:

Ensure that application 1 is started first by configuring its Deployment with appropriate resource requests, readiness probes, and startup delays if necessary.
Configure the readiness probes of application 2 to check for the readiness of application 1's endpoints before marking itself as ready.
Similarly, configure the readiness probes of application 3 to check for the readiness of application 2's endpoints before marking itself as ready.
Example YAML Configuration:

yaml
Copy code
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: <app1_image>
        readinessProbe:
          httpGet:
            path: /health
            port: 80
        # Add additional configuration as needed

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: app2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app2
  template:
    metadata:
      labels:
        app: app2
    spec:
      containers:
      - name: app2
        image: <app2_image>
        readinessProbe:
          httpGet:
            path: /health
            port: 80
        # Add additional configuration as needed
        dependsOn:
          - condition: "Ready"
            app: app1

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: app3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app3
  template:
    metadata:
      labels:
        app: app3
    spec:
      containers:
      - name: app3
        image: <app3_image>
        readinessProbe:
          httpGet:
            path: /health
            port: 80
        # Add additional configuration as needed
        dependsOn:
          - condition: "Ready"
            app: app2
Follow-up:

Check Mechanism for Application Dependencies:

Use readiness probes configured to check the specific endpoints of dependent applications before 
marking each application as ready.
Implement retry and backoff policies in readiness probes to handle transient failures gracefully 
and prevent unnecessary restarts or rescheduling.
Incorporating Check Mechanism with CD Tools:

Incorporate readiness probe configurations into the GitOps repository alongside the application deployment manifests.
Utilize GitOps tools like Argo CD, Flux, or Jenkins X to automatically apply changes to Kubernetes 
clusters based on changes in the Git repository.
Ensure that CD pipelines include validation steps to verify the correctness of readiness probe 
configurations and dependency requirements before applying updates to the cluster.




Question 6 : Please explain your understanding of multilayer switches and what Layer 4 LAN switching does.
Follow up:
1. Which component of Kubernetes using this technology and how ?



An Ingress controller is responsible for managing inbound traffic to Kubernetes services. 
It acts as a Layer 7 (application layer) load balancer, 
routing external traffic to the appropriate services within the cluster based on rules defined in Ingress resources.


Layer 4 Load Balancing:

Ingress controllers often rely on Layer 4 load balancing to distribute incoming traffic across
multiple backend pods or nodes.
Layer 4 load balancers operate at the transport layer (TCP/UDP) and make routing decisions 
based on network information such as IP addresses and port numbers.

Network Policies:

Kubernetes Network Policies are used to define rules for controlling traffic between pods or 
services within the cluster.
Network Policies operate at Layer 3 (network layer) and Layer 4 (transport layer) of the OSI model,
allowing administrators 
to control traffic based on IP addresses, ports, and protocols

Service Load Balancing:

Kubernetes Services provide an abstraction for accessing groups of pods or backend instances.
Service load balancing, implemented by Kubernetes Services, can operate at both Layer 4 (TCP/UDP load balancing) and 
Layer 7 (HTTP load balancing) depending on the type of Service and configuration.


Ingress Resource Routing:

Kubernetes Ingress resources define rules for routing external HTTP and HTTPS traffic to specific 
Services within the cluster.
Ingress controllers implement these routing rules and perform Layer 7 (application layer) 
load balancing based on HTTP/HTTPS headers and paths


While Ingress controllers primarily operate at Layer 7 for HTTP/HTTPS traffic routing, they may 
interact with multilayer 
switches or hardware load balancers operating at Layer 4 for traffic distribution and load balancing 
across backend pods or nodes. 
This Layer 4 functionality ensures efficient handling of network traffic and optimal utilization of cluster resources



Question 7 : You have 3 applications that make a solution. UI application, business application and database application. 
All the three applications are installable stand alone and connect with one another via configurations and endpoints. 
UI application needs 100MB and 1CPU, business application needs 200MB and 2CPU while the database application needs 
300MB and 3CPU for every 1000 users payload. So think of this as 1:2:3 ratio in terms of resources.

How will you design deployment the solution on the following infrastructures and also make the sure the 
resource ratios are maintained:
1. On VM
2. On docker setup
3. On k8s setup

Follow up:
Design the most optional solution, keeping the production cost as priority.


To deploy the solution across different infrastructures while maintaining resource ratios,
we can follow these approaches:

On VM:

Deploy each application on separate VM instances.
Allocate resources according to the specified ratios: UI (100MB, 1CPU), Business (200MB, 2CPU), 
Database (300MB per 1000 users, 3CPU per 1000 users).
Adjust the number of VM instances for each application based on resource requirements and expected user load.
On Docker Setup:

Containerize each application using Docker.
Define resource constraints in Docker Compose or Dockerfiles for each container
to ensure the specified ratios are maintained.
Use Docker Swarm for orchestration if necessary, ensuring appropriate resource allocation for each service.
On Kubernetes (k8s) Setup:

Create Kubernetes Deployments or StatefulSets for each application.
Define resource requests and limits in the pod specifications to ensure the desired ratios are maintained.
Utilize Horizontal Pod Autoscaling (HPA) for the database application based on user load to dynamically adjust resources.
Use Kubernetes' built-in scheduler to distribute pods across nodes based on available resources and 
resource requests/limits.
Follow-up:

Most Cost-Optimal Solution:
Considering production cost as a priority, the following adjustments can be made:

On VM:

Utilize VMs with appropriate sizing for each application but opt for cost-efficient instance types, 
such as reserved instances or spot instances, based on workload characteristics and usage patterns.
Implement auto-scaling policies to scale VM instances based on resource utilization and demand 
to minimize over-provisioning and reduce costs during periods of low activity.
On Docker Setup:

Optimize Docker container images to reduce resource footprint while maintaining functionality.
Utilize container orchestration platforms like Docker Swarm or Kubernetes to efficiently manage 
resource allocation and scaling based on workload demands, ensuring cost-effective utilization of resources.
On Kubernetes (k8s) Setup:

Leverage Kubernetes' native features such as Horizontal Pod Autoscaling (HPA) and Cluster Autoscaler 
to dynamically adjust resources based on workload requirements, thus optimizing resource utilization 
and reducing costs.
Implement resource quotas and limits to prevent over-provisioning and ensure efficient resource 
allocation across applications.
Consider utilizing managed Kubernetes services provided by cloud providers, which often offer 
cost-effective pricing models and automation for infrastructure management.
By adopting these cost optimization strategies across different deployment environments, 
you can ensure efficient resource utilization while minimizing production costs.




Question 8 : One of the main activity for devops enginner is to maintain the software versions of the platform. 
However, updating platform without having a downtime or customer impact is hard to achive. How are 
you updating your platform ?
How will you design a update statergy for k8s platform.

Follow up:
How do you plan for fail safe migration.
1. For example, during the update, the updation makes platform unstable or unresposive.
2. Migrating from x version to x+1 version, how will you evalivate that the plugings ( network, storage, deamons ) 
wont break the updation.


Updating a platform such as Kubernetes without causing downtime or customer impact requires careful 
planning and execution. Here's how you could design an update strategy for a Kubernetes platform:

Cluster Configuration Management: Use infrastructure as code (IaC) tools like Terraform or 
Kubernetes manifests stored in version control to manage cluster configuration. This ensures 
consistency and reproducibility across updates.

Test Environment: Maintain a staging or test environment that mirrors the production environment. 
Test updates thoroughly in this environment before applying them to production. 
Automated testing tools like Kubernetes e2e tests, integration tests, and chaos 
engineering practices can help identify potential issues.

Rolling Updates: Kubernetes supports rolling updates out of the box. By default, 
when you update a Deployment or StatefulSet, Kubernetes gradually replaces old pods with new ones, 
ensuring that the application remains available throughout the update process.

Canary Deployments: Implement canary deployments to gradually shift traffic from the old 
version to the new version. Monitor key metrics like latency, error rates, and resource 
utilization during the canary phase. If issues arise, rollback the update before it impacts all users.

Blue-Green Deployments: Set up blue-green deployments where you have two identical production 
environments (blue and green). Update one environment while keeping the other active. 
Once the update is validated, switch traffic to the updated environment.

Backup and Restore: Ensure you have a robust backup and restore strategy in place. 
Take regular backups of critical data and configurations, and validate 
the restore process periodically.

Health Checks and Probes: Configure appropriate readiness and liveness probes for your 
applications to ensure Kubernetes can detect and handle unhealthy containers effectively during updates.

Monitoring and Observability: Use monitoring tools like Prometheus, 
Grafana, and Kubernetes-native monitoring solutions to track cluster health, resource usage, 
and application performance. Set up alerts to notify you of any anomalies during the update process.

Fail-Safe Migration Planning:

Rollback Plan: Always have a rollback plan in place. Document steps to revert to the 
previous version quickly in case of issues. Automate rollback procedures where possible to minimize downtime.

Incremental Updates: Break down updates into smaller, incremental changes rather than 
one big update. This reduces the risk of catastrophic failures and makes troubleshooting easier.

Compatibility Testing: Before migrating to a new Kubernetes version, thoroughly test compatibility with 
plugins, network configurations, storage solutions, and other dependencies. Use tools like Kubeadm upgrade 
to help automate and validate the upgrade process.

Canary Testing: Conduct canary testing in production-like environments to validate that the update 
won't break critical functionalities. Monitor closely during this phase and be prepared to halt or roll back if necessary.

Communication and Coordination: Ensure clear communication and coordination among all stakeholders 
involved in the update process. This includes developers, operators, and any third-party vendors 
providing plugins or integrations.

By following these strategies and incorporating fail-safe migration planning, you can minimize 
the risk of downtime and customer impact while keeping your Kubernetes platform up to date with the 
latest features and security patches.



$#    Stores the number of command-line arguments that 
      were passed to the shell program.
      $# represents the number of arguments
      
$?    Stores the exit value of the last command that was 
      executed.
$0    Stores the first word of the entered command (the 
      name of the shell program).
$*    Stores all the arguments that were entered on the
      command line ($1 $2 ...).
"$@"  Stores all the arguments that were entered
      on the command line, individually quoted ("$1" "$2" ...).
      $@ expands to the positional arguments passed from the caller to either a function or a script
      
      $* means all the arguments passed to the script or function, split by word. 
      It is usually wrong and should be replaced by "$@" , which separates the arguments properly.
      
      
 $! - shows last process ID which has started in background
 $$ is the process id of the shell in which your script is running.
 
$0 is the name of the script itself, $1 is the first argument

"#!" is an operator called shebang which directs the script to the interpreter location.

The $- variable contains the shell's flags currently active in your terminal. 
These flags determine how your shell will function for you.
