kubectl config use-context
kubectl config --kubeconfig=/root/my-kube-config use-context research
kubectl config --kubeconfig=/root/my-kube-config current-context


openssl x509 -in apiserver.crt --text -noout
kubectl get pods --as dev-user


cat k.yml 
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - create 
  - delete
  
  

cat k1.yml 
---  
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user 
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role 
  name: developer
  apiGroup: rbac.authorization.k8s.io
  
  
 Role Creation
  
 Create a Role named "pod-reader" that allows users to perform get, watch and list on pods:
 kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
 
 Create a Role named "pod-reader" with resourceNames specified:
 kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
 
 Create a Role named "foo" with apiGroups specified:
 kubectl create role foo --verb=get,list,watch --resource=replicasets.apps
 
 Create a Role named "foo" with subresource permissions:
 kubectl create role foo --verb=get,list,watch --resource=pods,pods/status

 Create a Role named "my-component-lease-holder" with permissions to get/update a resource with a specific name:
 kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component
 
 https://kubernetes.io/docs/reference/access-authn-authz/rbac/
 

RoleBinding

kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme

kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme

kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme

ClusterRole
Create a ClusterRole named "pod-reader" that allows user to perform get, watch and list on pods:
kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods

kubectl create clusterrole node-admin --verb=get,watch,list,create,delete --resource=nodes
kubectl create clusterrolebinding node-admin-binding --clusterrole=node-admin --user=michelle

kubectl create clusterrole storage-admin --verb=get,watch,list,create,delete --resource=persistentvolumes,storageclasses


kubectl create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin  --user=michelle



Create a ClusterRole named "pod-reader" with resourceNames specified:

kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod

Create a ClusterRole named "foo" with apiGroups specified:

kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps

Create a ClusterRole named "foo" with subresource permissions:

kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status

Create a ClusterRole named "foo" with nonResourceURL specified:
kubectl create clusterrole "foo" --verb=get --non-resource-url=/logs/*

Create a ClusterRole named "monitoring" with an aggregationRule specified:
kubectl create clusterrole monitoring --aggregation-rule="rbac.example.com/aggregate-to-monitoring=true"

kubectl create clusterrolebinding 

Across the entire cluster, grant the permissions in the "cluster-admin" ClusterRole to a user named "root":
kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root

Across the entire cluster, grant the permissions in the "system:node-proxier" ClusterRole to a user named "system:kube-proxy":
kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy

Across the entire cluster, grant the permissions in the "view" ClusterRole to a service account named "myapp" in the namespace "acme":
kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp


etcdctl snapshot save --endpoints= --cacert= --cert= --key=

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db



ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-bkp \
snapshot restore /opt/snapshot-pre-boot.db





kubectl -n ingress-space create service ingress 

kubectl create nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]

kubectl create service nodeport myservice --node-port=31000 --tcp=3000:80

kubectl -n ingress-space expose deployment ingress-controller --port=80 --target-port=80 --type=NodePort --selector=nginx-ingress

kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https

https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_expose/

kubectl -n ingress-space expose deployment ingress-controller \
--name=ingress --port=80 --target-port=80 --type=NodePort \
--selector=name=nginx-ingress \
--overrides  '{ "apiVersion": "v1","spec":{"ports":[{"port": 80,"protocol":"TCP","targetPort": 80,"nodePort": 30080}]}}'

apiVersion: v1
kind: Service
metadata:
  name: ingress-controller
  namespace: ingress-space
spec:
  ports:
  - nodePort: 30080
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    name: nginx-ingress
  type: NodePort
status:
  loadBalancer: {}


 - port: 80
    targetPort: 80
    protocol: TCP
    nodePort: 30080
    name: http



cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

kubectl set image deployment/frontend  simple-webapp=kodekloud/webapp-color:v2


kubectl run static-busybox --image=busybox --restart=Never --command sleep 300



ROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]


FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]






kubectl create secret docker-registry secret-tiger-docker \
  --docker-username=tiger \
  --docker-password=pass113 \
  --docker-email=tiger@acme.com \
  --docker-server=my-registry.example:5000
  
kubectl create secret docker-registry private-reg-cred \
--docker-server=myprivateregistry.com:5000 \ 
--docker-username=dock_user --docker-password=dock_password \
--docker-email=dock_user@myprivateregistry.com
  

containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred



kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
  
json query

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}'

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.users[*].name}"

kubectl get pv --sort-by=.spec.capacity.storage 

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage 

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" 

kubectl -n kube-system get po nginx -o jsonpath="{.spec.containers[*].image}"





kubectl drain node01 --ignore-daemonsets
kubectl uncordon node01


kubectl -n kube-system  create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 


kubectl label node node01 color=blue

kubectl create deployment blue --image=nginx --replicas=3

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue

