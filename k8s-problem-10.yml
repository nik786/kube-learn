nodes can experience failures such as crashes, network disruption or storage faults that makes it unavailable or puts it in 'Not Ready' 
condition. But how does the Controller knows about it â“ 
The chronology of events follow as below :
ğŸ‘‰ Kubelet provides various operational status updates and heartbeat information about the nodes 
ğŸ‘‰ It uses the Lease mechanism (k8s ver > 1.17) to report heartbeat  that uses ğ’“ğ’†ğ’ğ’†ğ’˜ğ‘»ğ’Šğ’ğ’† and ğ’‰ğ’ğ’ğ’…ğ’†ğ’“ğ‘°ğ’…ğ’†ğ’ğ’•ğ’Šğ’•ğ’š to represent the last time each node was updated
 ğŸ‘‰ Kubelet and Controller work asynchronously, one reporting and the other monitoring.
 ğŸ‘‰ ğ’ğ’ğ’…ğ’†ğ‘³ğ’†ğ’‚ğ’”ğ’†ğ‘«ğ’–ğ’“ğ’‚ğ’•ğ’Šğ’ğ’ğ‘ºğ’†ğ’„ğ’ğ’ğ’…ğ’” on kubelet determines how often the Lease object is updated; the calculated value times 0.25 is the actual seconds
 ğŸ‘‰ ğ’ğ’ğ’…ğ’†-ğ’ğ’ğ’ğ’Šğ’•ğ’ğ’“-ğ’‘ğ’†ğ’“ğ’Šğ’ğ’… and ğ’ğ’ğ’…ğ’†-ğ’ğ’ğ’ğ’Šğ’•ğ’ğ’“-ğ’ˆğ’“ğ’‚ğ’„ğ’†-ğ’‘ğ’†ğ’“ğ’Šğ’ğ’… on the Controller determine how often the Controller checks and how long until it's considered "NotReady"
ğŸ‘‰ By using a goroutine, the ğ˜®ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜•ğ˜°ğ˜¥ğ˜¦ğ˜ğ˜¦ğ˜¢ğ˜­ğ˜µğ˜© function is executed every ğ’ğ’ğ’…ğ’†-ğ’ğ’ğ’ğ’Šğ’•ğ’ğ’“-ğ’‘ğ’†ğ’“ğ’Šğ’ğ’… second to check the node's status
ğŸ‘‰ By default, it takes at least 40 seconds to detect a node failure while a pod can survive on a faulty node for 300 seconds!

hashtag#k8s utilizes the Taint/Toleration mechanism to achieve automated rescheduling of the pods to 'healthy' 
nodes when any of the worker nodes is in failing state 


One of the method of configuring hashtag#HighAvailability on hashtag#Kubernetes is configuring etcd (distributed db) under two modes:
ğŸ‘‰ ğ’ğ­ğšğœğ¤ğğ ğğ­ğœğ ğ­ğ¨ğ©ğ¨ğ¥ğ¨ğ ğ² - This is the default mode of bring-up of distributed database provided by etcd that is brought upon the control plane nodes. 
This topology couples the control planes and etcd members on the same nodes. 
ğŸ“Œ Pros 
 âœ… Simpler to set up than a cluster with external etcd nodes
 âœ… Easier for managing replication.
 âœ… Reduces potential latency and network-related issues as communication is internal
 ğŸ›‘ Cons
 â— Runs the risk of failed coupling, redundancy is compromised
 â—Co-locating etcd with other Kubernetes components may lead to resource contention, especially in environments with limited resources
 â—Limited Scalability due to by-design nature

ğŸ‘‰ ğ„ğ±ğ­ğğ«ğ§ğšğ¥ ğğ­ğœğ ğ­ğ¨ğ©ğ¨ğ¥ğ¨ğ ğ² - Here etcd resides on a separate nodes and each etcd host communicates with the kube-apiserver of each control plane node
ğŸ“Œ Pros 
 âœ… Decouples the control plane and etcd member, losing a control plane instance or an etcd member has less impact and does not affect the cluster redundancy 
 âœ… With an external etcd cluster, one have more control over its scalability independent of hashtag#k8s cluster, offering more flexibility 
 âœ… External etcd clusters can potentially offer better performance, especially in scenarios where the etcd workload is heavy or requires significant resources
 ğŸ›‘ Cons
 â— Managing an external etcd cluster adds complexity to the Kubernetes infrastructure
 â—Network issues or latency as it's external to the cluster 
 â—Security Concerns as exposing etcd to external networks introduces potential security risk


 
