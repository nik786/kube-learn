𝐔𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝𝐢𝐧𝐠 𝐜𝐨𝐦𝐦𝐨𝐧 𝐊𝐮𝐛𝐞𝐫𝐧𝐞𝐭𝐞𝐬 𝐏𝐨𝐝 𝐟𝐚𝐢𝐥𝐮𝐫𝐞s 𝐚𝐧𝐝 𝐭𝐡𝐞𝐢𝐫 𝐬𝐨𝐥𝐮𝐭𝐢𝐨𝐧𝐬 ☸ 🔭 

Pods can have startup and runtime errors.
📌 Startup errors include:
 ✅ ImagePullBackoff
 ✅ ImageInspectError
 ✅ ErrImagePull
 ✅ ErrImageNeverPull
 ✅ RegistryUnavailable
 ✅ InvalidImageName

📌 Runtime errors include:
 ✅ CrashLoopBackOff
 ✅ RunContainerError
 ✅ KillContainerError
 ✅ VerifyNonRootError
 ✅ RunInitContainerError
 ✅ CreatePodSandboxError
 ✅ ConfigPodSandboxError
 ✅ KillPodSandboxError
 ✅ SetupNetworkError
 ✅ TeardownNetworkError

❗𝑰𝒎𝒂𝒈𝒆𝑷𝒖𝒍𝒍𝑩𝒂𝒄𝒌𝑶𝒇𝒇
✍ This error appears when hashtag#k8s isn't able to retrieve the image for one of the hashtag#containers of the Pod.
There are three common culprits:
✅ The image name is invalid
✅ You specified a non-existing tag for the image.
✅ The image that you're trying to retrieve belongs to a private registry and the cluster doesn't have credentials to access it.
The first two cases can be solved by correcting the image name and tag.
For the last, one should add the credentials to your private registry in a Secret and reference it in the Pods

❗𝑹𝒖𝒏𝑪𝒐𝒏𝒕𝒂𝒊𝒏𝒆𝒓𝑬𝒓𝒓𝒐𝒓
✍ The error appears when the container is unable to start before application
Common causes:
✅ Mounting a not-existent volume such as ConfigMap or Secrets
✅ Mounting a read-only volume as read-write
More detailed aspect can be found by describing the 'failed' pod

❗𝑪𝒓𝒂𝒔𝒉𝑳𝒐𝒐𝒑𝑩𝒂𝒄𝒌𝑶𝒇𝒇
✍ If the container can't start, then hashtag#Kubernetes shows the CrashLoopBackOff message as a status.
Usually, a container can't start when:
✅ There's an error in the application that prevents it from starting.
✅ You misconfigured the container.
✅ The Liveness probe failed too many times.

❗𝑷𝒐𝒅𝒔 𝒊𝒏 𝒂 𝑷𝒆𝒏𝒅𝒊𝒏𝒈 𝒔𝒕𝒂𝒕𝒆
✍ Assuming that the scheduler component is running fine, here are the causes:
✅ The cluster doesn't have enough resources such as CPU and memory to run the Pod.
✅ The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota.
✅ The Pod is bound to a Pending PersistentVolumeClaim.
The best option is to inspect the Events section in the "kubectl describe"


 Git Web - Open builtin GUI
 ✅ Git Notes - Attach extra info to commits
 ✅ Git Bisect - Debug like a pro
 ✅ Git Grep - Search for anything
 ✅ Git Archive - Compress project for sharing
 ✅ Git Submodules - Import other repos into yours
 ✅ Git Bugreport - Compile bug report with system info
 ✅ Git Fsck - Verify and recover unreachable objects
 ✅ Git Stripspace - Remove trailing whitespaces
 ✅ Git Diff - Compare changes between two files
 ✅ Git Hooks - Execute script when a git command is run
 ✅ Git Blame - Show who wrote a given line
 ✅ Git Large File Storage - Store big files in git
 ✅ Git Garbage Collection - Optimize your git repo
 ✅ Git Show - Easily inspect any git object
 ✅ Git Describe - Give readable name based on last tag
 ✅ Git Tag - Create version tags at specific points
 ✅ Git Reflog - List all git actions made on a repo
 ✅ Git Log - View commit log, and branch diagrams
 ✅ Git Cherry Pick - Pull a feature into your branch
 ✅ Git Switch - Quickly jump between branches


Demystifying the path of a DNS query in hashtag#kubernetes 🤔 ☸ 
How a requesting pod gets a correct IP address from hashtag#DNS Server ❓ 

➡️ 𝘞𝘩𝘦𝘯 𝘢 𝘱𝘰𝘥 𝘱𝘦𝘳𝘧𝘰𝘳𝘮𝘴 𝘢 𝘋𝘕𝘚 𝘭𝘰𝘰𝘬𝘶𝘱, 𝘵𝘩𝘦 𝘲𝘶𝘦𝘳𝘺 𝘪𝘴 𝘧𝘪𝘳𝘴𝘵 𝘴𝘦𝘯𝘵 𝘵𝘰 𝘵𝘩𝘦 𝘭𝘰𝘤𝘢𝘭 𝘋𝘕𝘚 𝘳𝘦𝘴𝘰𝘭𝘷𝘦𝘳 𝘪𝘯 𝘵𝘩𝘦 𝘱𝘰𝘥
➡️ 𝘛𝘩𝘪𝘴 𝘳𝘦𝘴𝘰𝘭𝘷𝘦𝘳 𝘶𝘴𝘦𝘴 𝘵𝘩𝘦 𝘳𝘦𝘴𝘰𝘭𝘷.𝘤𝘰𝘯𝘧 𝘤𝘰𝘯𝘧𝘪𝘨𝘶𝘳𝘢𝘵𝘪𝘰𝘯 𝘧𝘪𝘭𝘦 𝘸𝘩𝘦𝘳𝘦 𝘵𝘩𝘦 "𝘯𝘰𝘥𝘦𝘭𝘰𝘤𝘢𝘭𝘥𝘯𝘴" 𝘴𝘦𝘳𝘷𝘦𝘳 𝘪𝘴 𝘴𝘦𝘵 𝘶𝘱 𝘢𝘴 𝘵𝘩𝘦 𝘥𝘦𝘧𝘢𝘶𝘭𝘵 𝘳𝘦𝘤𝘶𝘳𝘴𝘪𝘷𝘦 𝘋𝘕𝘚 𝘳𝘦𝘴𝘰𝘭𝘷𝘦𝘳, 𝘸𝘩𝘪𝘤𝘩 𝘢𝘤𝘵𝘴 𝘢𝘴 𝘢 𝘤𝘢𝘤𝘩𝘦.
➡️ 𝘋𝘕𝘚 𝘴𝘦𝘳𝘷𝘦𝘳 𝘥𝘦𝘵𝘦𝘳𝘮𝘪𝘯𝘦𝘴 𝘵𝘩𝘦 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴 𝘣𝘺 𝘤𝘰𝘯𝘴𝘶𝘭𝘵𝘪𝘯𝘨 𝘵𝘩𝘦 𝘒𝘶𝘣𝘦𝘳𝘯𝘦𝘵𝘦𝘴 𝘴𝘦𝘳𝘷𝘪𝘤𝘦 𝘳𝘦𝘨𝘪𝘴𝘵𝘳𝘺
➡️ 𝘛𝘩𝘪𝘴 𝘳𝘦𝘨𝘪𝘴𝘵𝘳𝘺 𝘤𝘰𝘯𝘵𝘢𝘪𝘯𝘴 𝘢 𝘮𝘢𝘱𝘱𝘪𝘯𝘨 𝘰𝘧 𝘴𝘦𝘳𝘷𝘪𝘤𝘦 𝘯𝘢𝘮𝘦𝘴 𝘵𝘰 𝘵𝘩𝘦𝘪𝘳 𝘤𝘰𝘳𝘳𝘦𝘴𝘱𝘰𝘯𝘥𝘪𝘯𝘨 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴𝘦𝘴 𝘢𝘭𝘭𝘰𝘸𝘪𝘯𝘨 𝘵𝘩𝘦 𝘤𝘭𝘶𝘴𝘵𝘦𝘳 𝘋𝘕𝘚 𝘴𝘦𝘳𝘷𝘦𝘳 𝘵𝘰 𝘳𝘦𝘵𝘶𝘳𝘯 𝘵𝘩𝘦 𝘤𝘰𝘳𝘳𝘦𝘤𝘵 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴 𝘵𝘰 𝘵𝘩𝘦 𝘳𝘦𝘲𝘶𝘦𝘴𝘵𝘪𝘯𝘨 𝘱𝘰𝘥
➡️ 𝘛𝘩𝘪𝘴 𝘳𝘦𝘨𝘪𝘴𝘵𝘳𝘺 𝘤𝘰𝘯𝘵𝘢𝘪𝘯𝘴 𝘢 𝘮𝘢𝘱𝘱𝘪𝘯𝘨 𝘰𝘧 𝘴𝘦𝘳𝘷𝘪𝘤𝘦 𝘯𝘢𝘮𝘦𝘴 𝘵𝘰 𝘵𝘩𝘦𝘪𝘳 𝘤𝘰𝘳𝘳𝘦𝘴𝘱𝘰𝘯𝘥𝘪𝘯𝘨 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴𝘦𝘴 𝘢𝘭𝘭𝘰𝘸𝘪𝘯𝘨 𝘵𝘩𝘦 𝘤𝘭𝘶𝘴𝘵𝘦𝘳 𝘋𝘕𝘚 𝘴𝘦𝘳𝘷𝘦𝘳 𝘵𝘰 𝘳𝘦𝘵𝘶𝘳𝘯 𝘵𝘩𝘦 𝘤𝘰𝘳𝘳𝘦𝘤𝘵 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴 𝘵𝘰 𝘵𝘩𝘦 𝘳𝘦𝘲𝘶𝘦𝘴𝘵𝘪𝘯𝘨 𝘱𝘰𝘥
➡️ 𝘞𝘩𝘦𝘯 𝘢 𝘴𝘦𝘳𝘷𝘪𝘤𝘦 𝘪𝘴 𝘤𝘳𝘦𝘢𝘵𝘦𝘥 𝘪𝘯 𝘒𝘶𝘣𝘦𝘳𝘯𝘦𝘵𝘦𝘴, 𝘵𝘩𝘦 𝘤𝘭𝘶𝘴𝘵𝘦𝘳 𝘋𝘕𝘚 𝘴𝘦𝘳𝘷𝘦𝘳 𝘤𝘳𝘦𝘢𝘵𝘦𝘴 𝘢𝘯 '𝘈 𝘳𝘦𝘤𝘰𝘳𝘥' 𝘧𝘰𝘳 𝘵𝘩𝘦 𝘴𝘦𝘳𝘷𝘪𝘤𝘦
➡️ 𝘛𝘩𝘪𝘴 𝘳𝘦𝘤𝘰𝘳𝘥 𝘮𝘢𝘱𝘴 𝘵𝘩𝘦 𝘴𝘦𝘳𝘷𝘪𝘤𝘦'𝘴 𝘋𝘕𝘚 𝘯𝘢𝘮𝘦 𝘵𝘰 𝘪𝘵𝘴 𝘐𝘗 𝘢𝘥𝘥𝘳𝘦𝘴𝘴 𝘢𝘭𝘭𝘰𝘸𝘪𝘯𝘨 𝘱𝘰𝘥𝘴 𝘵𝘰 𝘢𝘤𝘤𝘦𝘴𝘴 𝘵𝘩𝘦 𝘴𝘦𝘳𝘷𝘪𝘤𝘦 𝘶𝘴𝘪𝘯𝘨 𝘪𝘵𝘴 𝘋𝘕𝘚 𝘯𝘢𝘮𝘦.


𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐝𝐞𝐛𝐮𝐠 (https://lnkd.in/gj8aJXpr)
✍ 'Out-of-tree' solution for connecting to and troubleshooting an existing, running, 'target' container in an existing pod in 
a hashtag#k8s cluster. The target container may have a shell and busybox utils and hence provide some debug capability

✅ 𝐰𝐢𝐧𝐝𝐨𝐰𝐬-𝐝𝐞𝐛𝐮𝐠 (https://lnkd.in/g9RYPDrK)
✍ Launches a hashtag#Windows host process pod with debugging tools that gives access to the node

✅ 𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐯𝐢𝐞𝐰-𝐚𝐥𝐥𝐨𝐜𝐚𝐭𝐢𝐨𝐧𝐬 (https://lnkd.in/gnZ5pdxQ)
✍ Lists allocations for resources (cpu, memory, gpu,...) as defined into the manifest of nodes and running pods

✅ 𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐭𝐫𝐞𝐞 (https://lnkd.in/gZJPyqT4)
✍ Browse hashtag#k8s object hierarchies as a tree

✅ 𝐤𝐮𝐛𝐞𝐭𝐚𝐩 (https://lnkd.in/gWdwfzDS)
✍ Enables an operator to easily deploy intercepting proxies for Kubernetes Services to debug failed network connections

✅ 𝐤𝐬𝐭𝐫𝐚𝐜𝐞 (https://lnkd.in/gpYrJXqT)
✍ Collects strace data from hashtag#Pods running in Kubernetes cluster. Watches and reviews system-calls from processes running inside Pods

✅ 𝐤𝐮𝐛𝐞𝐬𝐩𝐲 (https://lnkd.in/gZ65QuQj)
✍ Debug a running pod by creating a short-lived spy container, using specified image containing all the required debugging 
tools, to "spy" the target container by joining its OS namespaces

✅ 𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐬𝐢𝐜𝐤-𝐩𝐨𝐝𝐬 (https://lnkd.in/gQ3fPttG)
✍ Diagnosis running pods that are sick!

✅ 𝐤𝐭𝐨𝐩 (https://lnkd.in/g_BXeBCS)
✍ A 'top' like tool that that displays useful metrics information about nodes, pods, and other workload resources running in a Kubernetes cluster

✅ 𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐰𝐢𝐧𝐝𝐮𝐦𝐩𝐬 (https://lnkd.in/gJDcDwv8)
✍ Network traffic capture anaylzer in hashtag#AKS Windows Nodes

✅ 𝐤𝐮𝐛𝐞𝐜𝐭𝐥-𝐠𝐫𝐚𝐩𝐡 (https://lnkd.in/gpQeTqwm)
✍ Visualizes Kubernetes resources and relationships!!


Often times, we end up removing finalizers forcefully to removes resources stuck in "terminating" state.. 
Is this a good idea? Let's understand knowing Finalizers first and it's impact on resources it manages 
📌 𝑭𝒊𝒏𝒂𝒍𝒊𝒛𝒆𝒓𝒔 are namespaced keys that tell Kubernetes to wait until specific conditions are met before it 
fully deletes resources marked for deletion
📌 The most common finalizers are linked to Namespaces, Persistent Volume & Persistent Volume Claim
 ➡ kubernetes.io /pv-protection
 ➡ kubernetes.io /pvc-protection
 ➡ spec.finalizers => namespaces

🤔 So what happens when a request comes to api-server to delete an object that has finalizers specified for it 
 ✅ The API server handling the delete request notices the values in the finalizers field 
 ✅ Modifies the object to add a 𝒎𝒆𝒕𝒂𝒅𝒂𝒕𝒂.𝒅𝒆𝒍𝒆𝒕𝒊𝒐𝒏𝑻𝒊𝒎𝒆𝒔𝒕𝒂𝒎𝒑 field with the time the deletion is started
 ✅ Prevents the object from being removed until all items are removed from its 𝒎𝒆𝒕𝒂𝒅𝒂𝒕𝒂.𝒅𝒆𝒍𝒆𝒕𝒊𝒐𝒏𝑻𝒊𝒎𝒆𝒔𝒕𝒂𝒎𝒑 field
 ✅ Returns a 202 status code (HTTP "Accepted")
 ✅ When the finalizers field is emptied, an object with a deletionTimestamp field set is automatically deleted

📌 Finalizers removes objects gracefully, while if we attempt to remove forcefully, this may 
leave dangling resources in the cluster (such as LB, volumes on storage) that can incur significant cost to the operator !!

Therefore, let's think "twice" before using '--force --grace-period=0' or removing finalizers from resources. 
There might be situations when it's OK to ignore finalizer, but as a best pracrtise, investigate before using the nuclear solution 
and be aware of possible consequences as doing so might hide a systemic problem in your cluster.


𝐓𝐞𝐫𝐫𝐚𝐠𝐫𝐮𝐧𝐭 (https://lnkd.in/gHdQhFXt)
🔎 Thin wrapper that simplifies the configuration of Terraform deployments and supports code reuse

⚒ 𝐓𝐞𝐫𝐫𝐚𝐭𝐞𝐬𝐭 (https://lnkd.in/gU9gjbPW)
🔎 Testing framework for infrastructure code written in Go, facilitating automated testing and validation of Terraform code and hashtag#infrastructure

📍 𝐓𝐅𝐋𝐢𝐧𝐭 (https://lnkd.in/gn4QBzTM)
🔎 Terraform linter that analyzes code for errors, best practices and security issues, ensuring compliance with guidelines

🌊 𝐃𝐫𝐢𝐟𝐭𝐜𝐭𝐥 (https://lnkd.in/g74c8BSk)
🔎 Detects infrastructure drift by comparing actual resources with Terraform configuration, identifying changes or drift in attributes

🛡 𝐓𝐞𝐫𝐫𝐚𝐬𝐜𝐚𝐧 (https://lnkd.in/gZkxgBrf)
🔎 Security-focused scanner that detects risks, compliance violations and misconfigurations in hashtag#TF code

 ♻ 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦𝐞𝐫 (https://lnkd.in/gVBdWgKb)
 🔎 Generates Terraform configurations from existing infrastructure resources, facilitating management and reproducibility

🇸🇦 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦 𝐂𝐨𝐦𝐩𝐥𝐢𝐚𝐧𝐜𝐞 (https://lnkd.in/g7C5fQep)
🔎 Security and compliance scanner for Terraform code, allowing you to define and verify policies as code

🔐 𝐓𝐟𝐬𝐞𝐜 (https://lnkd.in/gSKxczHk)
🔎 Security-focused static analysis tool for Terraform code, providing recommendations and best practices for secure infrastructure

🚀 𝐓𝐞𝐫𝐫𝐚𝐬𝐩𝐚𝐜𝐞 (https://lnkd.in/gPxm4Pu4)
🔎 Framework that simplifies the development and deployment of Terraform infrastructure, offering automated module generation and integrated testing

What are your thoughts on these tools? 
Btw, I found this amazing tool Taplio that tracks best performing posts on LinkedIn of people in my industry, allowing 
me to dissect and emulate with ease on the contents I want to choose. Give it a try to discover post ideas, find top-notch 
niche content in Cloud & DevOps while boosting your LinkedIn post engagement simultaneously. 
