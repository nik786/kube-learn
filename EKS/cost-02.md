

# How We Cut Kubernetes Costs by 60% Without Downtime

Cloud costs were skyrocketing, and after a deep dive, we found hidden inefficiencies bleeding our budget.

## üî• Top Kubernetes Cost Culprits

| Cost Issue                      | Problem Description |
|----------------------------------|---------------------|
| **Idle Workloads**              | Running 24/7 even when not in use. |
| **Over-Provisioned CPU & Memory** | Wasting compute power. |
| **Unoptimized Autoscaling**      | Keeping expensive nodes active unnecessarily. |
| **Orphaned Resources**          | Persistent Volumes, Load Balancers, and Zombie Pods. |
| **Mismanaged Spot Instances**   | Unexpected evictions & higher on-demand costs. |
| **Excessive Network Egress Charges** | High cross-region data transfer costs. |





## üîç How We Fixed It & Slashed Costs by 60%

| Optimization Strategy            | Implementation |
|----------------------------------|---------------|
| **Smarter Autoscaling**         | ‚úÖ Replaced Cluster Autoscaler with **Karpenter** (Faster & cost-aware node provisioning). <br> ‚úÖ Used **Vertical Pod Autoscaler (VPA)** to adjust CPU/memory requests automatically. <br> ‚úÖ Optimized **Horizontal Pod Autoscaler (HPA)** for dynamic scaling. |
| **Scheduled & On-Demand Workloads** | ‚úÖ Used **KEDA** to spin up workloads only when needed. <br> ‚úÖ Moved non-critical workloads to **Argo Workflows** to reduce long-running container costs. <br> ‚úÖ **Paused Dev/Test clusters** after work hours using automation. |
| **Cleaning Up Wasted Resources** | ‚úÖ Ran `kubectl top` & **Kubecost** to find & remove over-provisioned workloads. <br> ‚úÖ Created a **Garbage Collector Controller** to delete: <br> üîπ Orphaned PVs & PVCs (Saved ~$2,000/month). <br> üîπ Unused Load Balancers & Ingresses. <br> üîπ Zombie Services & Stale Helm Releases. |
| **Network Cost Optimization**   | ‚úÖ Reduced **cross-region traffic** by keeping microservices within the same AZ. <br> ‚úÖ Used **Cilium** for service-to-service communication, cutting unnecessary egress charges. <br> ‚úÖ Optimized **Load Balancers** with **Ingress NGINX & Internal LBs** to reduce external traffic costs. |
| **Smarter Spot Instance Management** | ‚úÖ Used **Karpenter** to prioritize Spot Instances while ensuring fallback to On-Demand only when needed. <br> ‚úÖ Implemented **Spot.io Ocean** to dynamically optimize instance type selection for better cost efficiency. |



| **Aspect**                     | **Argo Workflows**                                                     | **GitLab CI/CD / Jenkins**                                              |
|-------------------------------|------------------------------------------------------------------------|-------------------------------------------------------------------------|
| **Execution Model**           | Runs as Kubernetes-native jobs that start and stop as needed.         | Often requires long-running agents or runners, which stay active.      |
| **Resource Usage**            | Uses ephemeral pods; resources are freed once a step completes.       | Runners/agents may stay up even when idle, consuming resources.         |
| **Auto-Scaling**              | Leverages Kubernetes auto-scaling for efficient resource use.         | Needs manual setup or integration for auto-scaling.                    |
| **Billing Efficiency**        | Pay only for the time each pod runs.                                  | May incur extra costs for always-on infrastructure.                    |
| **Container Lifecycle**       | Containers are short-lived and shut down after tasks.                 | Containers (runners/agents) may run continuously.                      |



| **Aspect**                     | **Cilium**                                                            | **AWS VPC CNI**                                                      | **Calico**                                                          | **Weave**                                                           | **Kube-Router**                                                    | **Knitter**                                                       |
|-------------------------------|------------------------------------------------------------------------|----------------------------------------------------------------------|---------------------------------------------------------------------|----------------------------------------------------------------------|--------------------------------------------------------------------|--------------------------------------------------------------------|
| **Communication Path**        | eBPF-based, efficient in-kernel routing.                              | Uses ENIs, may route via NAT/gateway.                               | IP routing via iptables or eBPF.                                    | Overlay network using VXLAN.                                        | IPVS/LVS-based routing within the cluster.                        | Routes via SR-IOV, VLAN, or VxLAN‚Äîmulti-network aware.            |
| **Egress Cost Optimization**  | Minimizes egress by keeping traffic in-cluster or on-node.            | Possible egress charges for cross-AZ/VPC traffic.                   | Similar to VPC unless tuned for IP preservation.                    | Higher overhead; more chance of egress costs in overlays.          | Keeps traffic within the cluster; good cost efficiency.           | Can reduce egress by using optimized network paths.               |
| **Data Path Efficiency**      | Very efficient (bypasses kernel stack using eBPF).                    | Moderate; depends on ENI/IP handling.                               | Good with eBPF; slower with iptables.                              | Lower performance due to user-space and overlays.                  | High-speed kernel routing (IPVS) provides good efficiency.        | Depends on underlying NICs (e.g., SR-IOV can be very efficient). |
| **NAT & SNAT Avoidance**      | Avoids SNAT with identity-aware routing.                              | SNAT often required for inter-VPC or cross-AZ.                      | SNAT common unless explicitly configured.                          | Uses NAT heavily in overlay networks.                              | Typically avoids NAT using internal routing.                      | Flexible NAT policies; can avoid SNAT depending on mode.          |
| **Scalability & Cost Impact** | Scales efficiently with fewer IPs and ENIs; eBPF is lightweight.       | Limited by ENI/IP allocations; can increase instance size/cost.     | IP exhaustion can be an issue; needs tuning for large clusters.    | Less scalable; overlays increase latency and resource usage.       | Scales well; kernel-level forwarding is resource-efficient.       | Scales with hardware; complex but efficient in high-throughput.   |




| **Aspect**              | **Cilium**                                                                 | **Kube-Router**                                                              |
|-------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Technology Base**     | Uses eBPF (kernel-level) for fast and efficient networking and security.   | Uses IPVS for routing, iptables for network policy, and BGP for routing.    |
| **Performance**         | High performance with low latency due to kernel bypass (eBPF).              | Good performance using kernel networking stack (IPVS), but not as fast as eBPF. |
| **Security Features**   | Advanced security policies with identity-aware filtering and visibility.    | Basic Kubernetes network policies; lacks deep inspection or identity layers. |
| **Observability**       | Built-in tools like Hubble for real-time visibility and monitoring.         | Limited built-in observability; requires external tools.                    |
