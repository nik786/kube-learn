
PRACTICE TEST COMMANDS AND ARGUMENTS


1. Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. 
Modify the file ubuntu-sleeper-2.yaml.

Note: Only make the necessary changes. Do not modify the name.

Pod Name: ubuntu-sleeper-2
Command: sleep 5000


2.
Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!

Note: Only make the necessary changes. Do not modify the name.

Pod Name: ubuntu-sleeper-3
Command: sleep 1200

3.

Inspect the file Dockerfile given at /root/webapp-color directory. What command is run at container startup?

ROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]

4.
Inspect the two files under directory webapp-color-2. What command is run at container startup?

Assume the image was created from the Dockerfile in this folder.


5.
Inspect the two files under directory webapp-color-3. What command is run at container startup?

Assume the image was created from the Dockerfile in this folder.

6.

Create a pod with the given specifications. By default it displays a blue background. 
Set the given command line arguments to change it to green

Pod Name: webapp-green
Image: kodekloud/webapp-color
Command line arguments: --color=green



apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]





PRACTICE TEST ENV VARIABLES
1.
Create a new ConfigMap for the webapp-color POD. Use the spec given below.
ConfigName Name: webapp-config-map
Data: APP_COLOR=darkblue


2.
Update the environment variable on the POD to use the newly created ConfigMap
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.

Pod Name: webapp-color
EnvFrom: webapp-config-map

3.


PRACTICE TEST SECRETS
Create a new secret named db-secret with the data given below.
You may follow any one of the methods discussed in lecture to create the secret.

Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123

4.
Configure webapp-pod to load environment variables from the newly created secret.

Delete and recreate the pod if required.

Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret



PRACTICE TEST â€“ MULTI CONTAINER PODS

1.
Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis


2.

- env:
    - name: discovery.type
      value: single-node



Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.



Only add a new container. Do not modify anything else. Use the spec provided below.


Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator



PRACTICE TEST ROLLING UPDATES AND ROLLBACKS

Deployment Rolling Strategy

1.

Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2

Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2

2.

Change the deployment strategy to Recreate

Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2
Strategy: Recreate

3.

Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v3

4.
You are requested to change the URLs at which the applications are made available.
Make the video application available at /stream


Ingress: ingress-wear-watch
Path: /stream
Backend Service: video-service
Backend Service Port: 8080

5.


You are requested to add a new path to your ingress to make the food delivery application available to your customers.
Make the new application available at /eat.



Ingress: ingress-wear-watch
Path: /eat
Backend Service: food-service
Backend Service Port: 8080


6.

View the Payment application using the /pay URL in your browser.
Click on the Ingress tab above your terminal, if its not open already, and append /pay to the URL in the browser.


You are requested to make the new application available at /pay.
Identify and implement the best approach to making this application available
on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

Ingress Created
Path: /pay
Configure correct backend service
Configure correct backend port


7.

Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Create the ingress in the app-space namespace.

Ingress Created
Path: /wear
Path: /watch
Configure correct backend service for /wear
Configure correct backend service for /watch
Configure correct backend port for /wear service
Configure correct backend port for /watch service


8.

Let us now create a service to make Ingress available to external users.
Create a service following the given specs.


Name: ingress
Type: NodePort
Port: 80
TargetPort: 80
NodePort: 30080
Namespace: ingress-space
Use the right selector


PRACTICE TEST VIEW CERTIFICATE DETAILS

1.
Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server

Run the command cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for value of etcd-certfile flag.

2.
Identify the key used to authenticate kubeapi-server to the kubelet server
Look for kubelet-client-key option in the file /etc/kubernetes/manifests/kube-apiserver.yaml

3.
What is the Common Name (CN) configured on the Kube API Server Certificate?
openssl x509 -in file-path.crt -text -noout

4.
What is the name of the CA who issued the Kube API Server Certificate?
Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and look for issuer

5.
Which of the below alternate names is not configured on the Kube API Server Certificate?

Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and look at Alternative Names


6.
What is the Common Name (CN) configured on the ETCD Server certificate?
Run the command openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text and look for Subject CN.


7.
How long, from the issued date, is the Kube-API Server Certificate valid for?
Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and check on the Expiry date.

8.
How long, from the issued date, is the Root CA Certificate valid for?
File: /etc/kubernetes/pki/ca.crt

openssl x509 -in /etc/kubernetes/pki/ca.crt -text

9.
Kubectl suddenly stops responding to your commands. Check it out! 
Someone recently modified the /etc/kubernetes/manifests/etcd.yaml file
You are asked to investigate and fix the issue. Once you fix the issue 
wait for sometime for kubectl to respond. Check the logs of the ETCD container.

The certificate file used here is incorrect. It is set to /etc/kubernetes/pki/etcd/server-certificate.crt 
which does not exist. As we saw in the previous 
questions the correct path should be /etc/kubernetes/pki/etcd/server.crt.

Update the YAML file with the correct certificate path and wait for 
the ETCD pod to be recreated. wait for the kube-apiserver to get to a Ready state

KUBECONFIG TEST

1.
What is the current context set to in the my-kube-config file?
kubectl config current-context --kubeconfig my-kube-config

2.
I would like to use the dev-user to access test-cluster-1. 
Set the current context to the right one so I can do that.
Once the right context is identified, use the kubectl config use-context command.

To use that context, run the command: kubectl config --kubeconfig=/root/my-kube-config use-context research
To know the current context, run the command: kubectl config --kubeconfig=/root/my-kube-config current-context

3.
We don't want to have to specify the kubeconfig file option on each command. 
Make the my-kube-config file the default kubeconfig.

Replace the contents in the default kubeconfig file with the content from my-kube-config file.



3.

PRACTICE TEST ROLE BASED ACCESS CONTROLS

Inspect the environment and identify the authorization modes configured on the cluster.

kubectl describe pod kube-apiserver-controlplane -n kube-system and look for --authorization-mode.

4.
What actions can the kube-proxy role perform on configmaps?
Run the command: kubectl describe role -n kube-system kube-proxy and check under the Verbs column.

5.

A user dev-user is created. User's details have been added to the kubeconfig file. 
Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
Use the --as dev-user option with kubectl to run commands as the dev-user.

Run the command: kubectl get pods --as dev-user

6.

Create the necessary roles and role bindings required for the dev-user to create, list and 
delete pods in the default namespace.
Use the given spec:

Role: developer
Role Resources: pods
Role Actions: list
Role Actions: create
Role Actions: delete
RoleBinding: dev-user-binding
RoleBinding: Bound to dev-user

7.

The dev-user is trying to get details about the dark-blue-app pod in the blue namespace. 
Investigate and fix the issue.
We have created the required roles and rolebindings, but something seems to be wrong.

Run the command: kubectl edit role developer -n blue and correct the resourceNames field. 
You don't have to delete the role.

8.
Grant the dev-user permissions to create deployments in the blue namespace.
Remember to add for api group "apps"

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: blue
  name: dev-user-role
rules:
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-role-binding
  namespace: blue
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-user-role
  apiGroup: rbac.authorization.k8s.io


Test Service Account


SECURITY CONTEXTS

What is the user used to execute the sleep process within the ubuntu-sleeper pod?
In the current(default) namespace.

kubectl exec ubuntu-sleeper -- whoami

2.
Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.
Note: Only make the necessary changes. Do not modify the name or image of the pod.

3.
A Pod definition file named multi-pod.yaml is given. With what user are the processes in the web container started?
The pod is created with multiple containers and security contexts defined at the Pod and Container level.

The User ID defined in the securityContext of the container overrides the User ID in the POD.

4.


5.

Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
Note: Only make the necessary changes. Do not modify the name of the pod.

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
        
 6.
 Now update the pod to also make use of the NET_ADMIN capability.
 Note: Only make the necessary changes. Do not modify the name of the pod.


Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME
SecurityContext: Capability NET_ADMIN

metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]
        





1.

We decided to use a modified version of the application from an internal private registry. 
Update the image of the deployment to use a new image from myprivateregistry.com:5000
The registry is located at myprivateregistry.com:5000. 
Don't worry about the credentials for now. We will configure them in the upcoming steps.


2.
Create a secret object with the credentials required to access the registry.
Name: private-reg-cred
Username: dock_user
Password: dock_password
Server: myprivateregistry.com:5000
Email: dock_user@myprivateregistry.com

3.

Configure the deployment to use credentials from the new secret to pull images from the private registry

Edit deployment using kubectl edit deploy web command and add imagePullSecrets section. Use private-reg-cred.

4.






1.

Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file

---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Zq
  Q0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFR
  gpBQU9DQVE4QU1JSUJDZ0tDQVFFQXY4azZTTE9HVzcrV3JwUUhITnI2TGFROTJhVmQ1blNLaj
  R6UEhsNUlJYVdlCmJ4RU9JYkNmRkhKKzlIOE1RaS9hbCswcEkwR2xpYnlmTXozL2lGSWF3eGVXNFA3b
  DJjK1g0L0lqOXZQVC9jU3UKMDAya2ZvV0xUUkpQbWtKaVVuQTRpSGxZNDdmYkpQZDhIRGFuWHM3bnFoen
  VvTnZLbWhwL2twZUVvaHd5MFRVMAo5bzdvcjJWb1hWZTVyUnNoMms4dzV2TlVPL3BBdEk4VkRydUhCYzRxaHM3MDI1ZTZTUXFDeHUyOHNhTDh1blJQCkR6V2ZsNVpLcTVpdlJNeFQrcUo0UGpBL2pHV2d6QVliL1hDQXRrRVJyNlMwak9XaEw1Q0ErVU1BQmd5a1c5emQKTmlXbnJZUEdqVWh1WjZBeWJ1VzMxMjRqdlFvbndRRUprNEdoayt2SU53SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBQi94dDZ2d2EweWZHZFpKZ1k2ZDRUZEFtN2ZiTHRqUE15OHByTi9WZEdxN25oVDNUUE5zCjEwRFFaVGN6T21hTjVTZmpTaVAvaDRZQzQ0QjhFMll5Szg4Z2lDaUVEWDNlaDFYZnB3bnlJMVBDVE1mYys3cWUKMkJZTGJWSitRY040MDU4YituK24wMy9oVkN4L1VRRFhvc2w4Z2hOaHhGck9zRUtuVExiWHRsK29jQ0RtN3I3UwpUYTFkbWtFWCtWUnFJYXFGSDd1dDJveHgxcHdCdnJEeGUvV2cybXNqdHJZUXJ3eDJmQnErQ2Z1dm1sVS9rME4rCml3MEFjbVJsMy9veTdqR3ptMXdqdTJvNG4zSDNKQ25SbE41SnIyQkZTcFVQU3dCL1lUZ1ZobHVMNmwwRERxS3MKNTdYcEYxcjZWdmJmbTRldkhDNnJCSnNiZmI2ZU1KejZPMUU9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth




3.

What is the Condition of the newly created Certificate Signing Request object?

kubectl get csr


4.
Approve the CSR Request
kubectl certificate approve akshay


5.

How many CSR requests are available on the cluster?


6.

During a routine check you realized that there is a new CSR request in place. What is the name of this request?


7.
You are not aware of a request coming in. What groups is this CSR requesting access to?
Check the details about the request. Preferebly in YAML.

8.



NetWork Policy

Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
Use the spec given on the below. You might want to enable ingress traffic to the pod to test your rules in the UI.

Policy Name: internal-policy
Policy Type: Egress
Egress Allow: payroll
Payroll Port: 8080
Egress Allow: mysql
MySQL Port: 3306


Troubleshooting
PRACTICE TEST APPLICATION FAILURE


A simple 2 tier application is deployed in the alpha namespace. 
It must display a green web page on success. Click on the App tab 
at the top of your terminal to view your application. It is currently failed. 
Troubleshoot and fix the issue.

Stick to the given architecture. 
Use the same names and port numbers as given in the below architecture diagram. 
Feel free to edit, delete or recreate objects as necessary.

The service name used for the MySQL Pod is incorrect. According to the Architecture diagram, it should be mysql-service.
To fix this, first delete the current service: kubectl -n alpha delete svc mysql
Then create a new service with the following YAML file (or use imperative command):

apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: alpha
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql
      
      



The same 2 tier application is deployed in the beta namespace. It must display a green web page on success. 
Click on the App tab at the top of your terminal to view your application. 
It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and 
port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


If you inspect the mysql-service in the beta namespace, you will notice that 
the targetPort used to create this service is incorrect.
Compare this to the Architecture diagram and change it to 3306. 
Update the mysql-service as per the below YAML:






apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: beta
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql
      


The same 2 tier application is deployed in the gamma namespace. It must display a green web page on success.
Click on the App tab at the top of your terminal to view your application. 
It is currently failed. Troubleshoot and fix the issue.

Stick to the given architecture. Use the same names and port 
numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


If you inspect the mysql-service, you will see that that the selector used does not match the label on the mysql pod.
root@controlplane:~# kubectl -n gamma describe svc mysql-service | grep -i selector
Selector:          name=sql00001
root@controlplane:~#

root@controlplane:~# kubectl -n gamma describe pod mysql | grep -i label   
Labels:       name=mysql
root@controlplane:~#

As you can see the selector used is name=sql001 whereas it should be name=mysql.
Update the mysql-service to use the correct selector as per the below YAML:

apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: gamma
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql


Troubleshooting Test 4: The same 2 tier application is deployed in the delta namespace. 
It must display a green web page on success. Click on the App tab at the top of 
your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

Stick to the given architecture. Use the same names and port numbers as 
given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

Try accessing the web application from the browser using the tab called app. 
You will notice that it cannot connect to the MySQL database:

Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; 
DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.244.1.9' (using password: YES)

According to the architecture diagram, the DB_User should be root but it is set to sql-user in the webapp-mysql deployment.
Use the command kubectl -n delta edit deployments.apps webapp-mysql and update the environment variable as follows:

spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
          
          
         
Troubleshooting Test 5: The same 2 tier application is deployed in the epsilon namespace. 
It must display a green web page on success. Click on the App tab at the top of your terminal 
to view your application. It is currently failed. Troubleshoot and fix the issue.

Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. 
Feel free to edit, delete or recreate objects as necessary.       

If you inspect the environment variable called MYSQL_ROOT_PASSWORD, you will notice that the value is incorrect 
as compared to the architecture diagram:

root@controlplane:~# kubectl -n epsilon describe pod mysql  | grep MYSQL_ROOT_PASSWORD 
      MYSQL_ROOT_PASSWORD:  passwooooorrddd
root@controlplane:~#

Correct this by deleting and recreating the mysql pod with the correct environment variable as follows:

spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd
      
 Also edit the webapp-mysql deployment and make sure that the DB_User environment variable is set to root as follows:
 
 spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
          
 Once the objects are recreated, and you should be able to access the application.
 
 
 Troubleshooting Test 6: The same 2 tier application is deployed in the zeta namespace. 
 It must display a green web page on success. 
 Click on the App tab at the top of your terminal to view your application. 
 It is currently failed. Troubleshoot and fix the issue.

Stick to the given architecture. Use the same names and port numbers as given in 
the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.
      
There are a few things wrong in this setup:

1. If you inspect the web-service, you will see that the nodePort used is incorrect.
This service should be exposed on port 30081 and NOT 30088.

root@controlplane:~# kubectl -n zeta get svc web-service 
NAME          TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web-service   NodePort   10.102.190.212   <none>        8080:30088/TCP   3m1s
root@controlplane:~#


To correct this, delete the service and recreate it using the below YAML file:


```
apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: zeta
spec:
  ports:
  - nodePort: 30081
    port: 8080
    targetPort: 8080
  selector:
    name: webapp-mysql
  type: NodePort
  
  
   Also edit the webapp-mysql deployment and make sure that the 
   DB_User environment variable is set to root as follows:



spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
          
          
 The DB_Password used by the mysql pod is incorrect. Delete the current pod and 
 recreate with the correct environment variable as per the snippet below:  
 
 spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd


Practice Test - Control Plane Failure
The cluster is broken again. We tried deploying an application but it's not working. 
Troubleshoot and fix the issue.
Start looking at the deployments.

Run the command: kubectl get pods -n kube-system. Check the kube-scheduler manifest file and fix the issue.
The command run by the scheduler pod is incorrect. Here is a snippet of the YAML file.

spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    - --port=0
    
    
Even though the deployment was scaled to 2, the number of PODs does not seem to increase. 
Investigate and fix the issue. Inspect the component responsible for managing 
deployments and replicasets   

Run the command: kubectl get po -n kube-system and check the logs of 
kube-controller-manager pod to know the failure reason by running command: 
kubectl logs -n kube-system kube-controller-manager-controlplane
Then check the kube-controller-manager configuration file at 
/etc/kubernetes/manifests/kube-controller-manager.yaml and fix the issue

The configuration file specified (/etc/kubernetes/controller-manager-XXXX.conf)
does not exist.
Correct the path: /etc/kubernetes/controller-manager.conf



2

Even though the deployment was scaled to 2, the number of 
PODs does not seem to increase. Investigate and fix the issue.
Inspect the component responsible for managing deployments and replicasets.

Run the command: kubectl get po -n kube-system and check the 
logs of kube-controller-manager pod to know the failure reason by running command: 
kubectl logs -n kube-system kube-controller-manager-controlplane
Then check the kube-controller-manager configuration file at 
/etc/kubernetes/manifests/kube-controller-manager.yaml and fix the issue.


root@controlplane:/etc/kubernetes/manifests# kubectl -n kube-system logs kube-controller-manager-controlplane
Flag --port has been deprecated, see --secure-port instead.
I0725 07:25:16.842138       1 serving.go:331] Generated self-signed cert in-memory
stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory
root@controlplane:/etc/kubernetes/manifests# 

The configuration file specified (/etc/kubernetes/controller-manager-XXXX.conf) does not exist.
Correct the path: /etc/kubernetes/controller-manager.conf


3.
Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. 
But it's not happening.
Investigate and fix the issue.

Check the volume mount path in kube-controller-manager manifest file at /etc/kubernetes/manifests.
Just as we did in the previous question, inspect the logs of the kube-controller-managerpod:

root@controlplane:/etc/kubernetes/manifests# kubectl -n kube-system logs kube-controller-manager-controlplane
Flag --port has been deprecated, see --secure-port instead.
I0725 07:29:06.155330       1 serving.go:331] Generated self-signed cert in-memory
unable to load client CA file "/etc/kubernetes/pki/ca.crt": open /etc/kubernetes/pki/ca.crt: no such file or directory
root@controlplane:/etc/kubernetes/manifests# 


It appears the path /etc/kubernetes/pki is not mounted from the 
controlplane to the kube-controller-manager pod. If we inspect the 
pod manifest file, we can see that the incorrect hostPath is used for the volume:
WRONG


- hostPath:
      path: /etc/kubernetes/WRONG-PKI-DIRECTORY
      type: DirectoryOrCreate
      
CORRECT: ``yaml

hostPath: path: /etc/kubernetes/pki type: DirectoryOrCreate ``` Once the path is corrected, 
the pod will be recreated and our deployment should eventually scale up to 3 replicas      

3

kubelet has stopped running on node01 again. Since this is a systemd managed system, 
we can check the kubelet log by running journalctl. Here is a snippet showing the error with kubelet:

kubelet has stopped running on node01 again. Since this is a systemd managed system,
we can check the kubelet log by running journalctl. 
Here is a snippet showing the error with kubelet:

journalctl -u kubelet 

There appears to be a mistake path used for the CA certificate in the kubelet configuration.
This can be corrected by updating the file /var/lib/kubelet/config.yaml.
Once this is fixed, restart the kubelet service, (like we did in the previous question) 
and node01 should return back to a working state.


4.

Once again the kubelet service has stopped working. Checking the logs, we can see that this time, 
it is not able to reach the kube-apiserver

journalctl -u kubelet

As we can clearly see, kubelet is trying to connect to the API server 
on the controlplane node on port 6553. This is incorrect.
To fix, correct the port on the kubeconfig file used by the kubelet.

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data:
    --REDACTED---
    server: https://controlplane:6443
    
    
    
    
**Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. 
It must display a green web page on success. Click on the app tab at the top of 
your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
 
Do the services in triton namespace have a valid endpoint? If they do, check the kube-proxy and the weave logs.
Does the cluster have a Network Addon installed?

Install Weave using the link: 
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network

For example: 
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


**Troubleshooting Test 2:** The same 2 tier application is having issues again. 
It must display a green web page on success. Click on the app tab at the top of your terminal 
to view your application. It is currently failed. Troubleshoot and fix the issue
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. 
Feel free to edit, delete or recreate objects as necessary. 

The kube-proxy pods are not running. As a result the rules needed to allow 
connectivity to the services have not been created.
Check the logs of the kube-proxy pods kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
The configuration file /var/lib/kube-proxy/configuration.conf is not valid. 
The configuration path does not match the data in the ConfigMap. 
kubectl -n kube-system describe configmap kube-proxy shows 
that the file name used is config.conf which is mounted in the kube-proxy 
damonset pods at the path /var/lib/kube-proxy/config.conf
However in the DaemonSet for kube-proxy, the command used
to start the kube-proxy pod makes use of the path /var/lib/kube-proxy/configuration.conf.
Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pods.

Here is the snippet of the command to be run by the kube-proxy pods:

spec:
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)





PRACTISE TEST JSON PATH

1.
Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json

kubectl get nodes -o json > /opt/outputs/nodes.json

2.

Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json

kubectl get node node01 -o json > /opt/outputs/node01.json


3.

Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

4.

Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt.

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

5.

A kube-config file is present at /root/my-kube-config. Get the user names from it 
and store it in a file /opt/outputs/users.txt.
Use the command kubectl config view --kubeconfig=/root/my-kube-config to view the custom kube-config.

Use the command kubectl config view --kubeconfig=my-kube-config -o 
jsonpath="{.users[*].name}" > /opt/outputs/users.txt


6.

A set of Persistent Volumes are available. Sort them based on their capacity 
and store the result in the file /opt/outputs/storage-capacity-sorted.txt.

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

7.

That was good, but we don't need all the extra details. Retrieve just the first 2 columns of 
output and store it in /opt/outputs/pv-and-capacity-sorted.txt.
The columns should be named NAME and CAPACITY. Use the custom-columns option 
and remember, it should still be sorted as in the previous question.

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage
> /opt/outputs/pv-and-capacity-sorted.txt


8.

Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file 
and store the result in /opt/outputs/aws-context-name

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}"
> /opt/outputs/aws-context-name



Scheduling

Labels & Selectors
1.
We have deployed a number of PODs. They are labelled with tier, env and bu. 
How many PODs exist in the dev environment?
Use selectors to filter the output

kubectl get pods --selector env=dev --no-headers | wc -l

2.
How many PODs are in the finance business unit (bu)?
kubectl get pods --selector bu=finance --no-headers | wc -l

3.
How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
kubectl get all --selector env=prod --no-headers | wc -l

4.
Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
kubectl get all --selector env=prod,bu=finance,tier=frontend

5.
A ReplicaSet definition file is given replicaset-definition-1.yaml. 
Try to create the replicaset. There is an issue with the file. Try to fix it.



---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: front-end
   template:
     metadata:
       labels:
        tier: front-end
     spec:
       containers:
       - name: nginx
         image: nginx 


Taints & Tolerations:

1.
Do any taints exist on node01 node?
kubectl describe node node01 | grep -i taints

2.
Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
kubectl taint nodes node01 spray=mortein:NoSchedule

3.

Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.

Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running

---
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
    
4.

Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

5.







































