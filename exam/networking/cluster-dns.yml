---
Lecture: Cluster DNS
https://linuxacademy.com/cp/courses/lesson/course/4018/lesson/5/module/327

1. Which is default DNS plugin for Kubernetes??
   CoreDNS is now the new default DNS plugin for Kubernetes. 

2. What is the command to view the CoreDNS pods in the kube-system namespace:
   kubectl get pods -n kube-system

3.What is the command to View the CoreDNS deployment in your Kubernetes cluster:
  kubectl get deployments -n kube-system

4.What is the command to View the service that performs load balancing for the DNS server:
  kubectl get services -n kube-system

5.Write manifest for busybox
Spec for the busybox pod:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always


6. What is the command to view the resolv.conf file that contains the nameserver and search in DNS:
   kubectl exec -it busybox -- cat /etc/resolv.conf


7.What is the command to Look up the DNS name for the native Kubernetes service:
  kubectl exec -ti busybox -- nslookup [pod-ip-address].default.pod.cluster.local

8.What is the command to Look up a service in your Kubernetes cluster:
  kubectl exec -it busybox -- nslookup kube-dns.kube-system.svc.cluster.local

9.Get the logs of your CoreDNS pods:
  kubectl logs [coredns-pod-name]


10.Write YAML spec for a headless service:
apiVersion: v1
kind: Service
metadata:
  name: kube-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubserve2


11.YAML spec for a custom DNS pod:


apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0


---
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
DNS for Services and Pods

Introduction

Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to 
tell individual containers to use the DNS Service’s IP to resolve DNS names.



What things get DNS names?
Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name.
By default, a client Pod’s DNS search list will include the Pod’s own namespace and the cluster’s
default domain. 




Services

A records
“Normal” (not headless) Services are assigned a DNS A record for a name of the form 
my-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IP of the Service.

“Headless” (without a cluster IP) Services are also assigned a DNS A record for a name of the 
form my-svc.my-namespace.svc.cluster-domain.example. Unlike normal Services, this resolves to 
the set of IPs of the pods selected by the Service. Clients are expected to consume the set or 
else use standard round-robin selection from the set.




SRV records
SRV Records are created for named ports that are part of normal or Headless Services. 
For each named port, the SRV record would have the 
form _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example. 
For a regular service, this resolves to the port number and the domain name: 
my-svc.my-namespace.svc.cluster-domain.example. For a headless service, this
resolves to multiple answers, one for each pod that is backing the service, and 
contains the port number and the domain name of the pod of the form 
auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example.




Pods

Pod’s hostname and subdomain fields
Currently when a pod is created, its hostname is the Pod’s metadata.name value.

The Pod spec has an optional hostname field, which can be used to specify the Pod’s
hostname. When specified, it takes precedence over the Pod’s name to be the hostname of the pod.
For example, given a Pod with hostname set to “my-host”, the Pod will have its hostname set to “my-host”.

The Pod spec also has an optional subdomain field which can be used to specify its subdomain.
For example, a Pod with hostname set to “foo”, and subdomain set to “bar”, in namespace “my-namespace”,
will have the fully qualified domain name (FQDN) “foo.bar.my-namespace.svc.cluster-domain.example

Example:

apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo # Actually, no port is needed.
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  labels:
    name: busybox
spec:
  hostname: busybox-2
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox

What will be dns name resolution for the pod if the hostname of pod set to “busybox-1”
and the subdomain set to “default-subdomain”,a headless Service named “default-subdomain” 
in the same namespace??
pod will see its own FQDN as 
“busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example





Note: Because A records are not created for Pod names, hostname is required for the Pod’s 
A record to be created. A Pod with no hostname but with subdomain will only create the A record 
for the headless service (default-subdomain.my-namespace.svc.cluster-domain.example), pointing
to the Pod’s IP address. Also, Pod needs to become ready in order to have a record unless 
publishNotReadyAddresses=True is set on the Service.


Pod’s DNS Policy
DNS policies can be set on a per-pod basis. 

1. What is default dns policy??
The Pod inherits the name resolution configuration from the node that the 
pods run on. See related discussion for more details.

2.What “ClusterFirst“ dns policy??
Any DNS query that does not match the configured cluster domain suffix,
such as “www.kubernetes.io”, is forwarded to the upstream nameserver inherited from the node.

3. What is “ClusterFirstWithHostNet“ dns policy??
For Pods running with hostNetwork, you should explicitly set its DNS policy “ClusterFirstWithHostNet”.

4.What is “None“ dns policy??
It allows a Pod to ignore DNS settings from the Kubernetes environment. 
All DNS settings are supposed to be provided using the dnsConfig field in the Pod Spec.
See Pod’s DNS config subsection below.

Note: “Default” is not the default DNS policy. If dnsPolicy is not explicitly specified,
then “ClusterFirst” is used.
The example below shows a Pod with its DNS policy set to “ClusterFirstWithHostNet”
because it has hostNetwork set to true.

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet



Pod’s DNS Config
Pod’s DNS Config allows users more control on the DNS settings for a Pod.

The dnsConfig field is optional and it can work with any dnsPolicy settings.
However, when a Pod’s dnsPolicy is set to “None”, the dnsConfig field has to be specified.

Below are the properties a user can specify in the dnsConfig field:

nameservers: a list of IP addresses that will be used as DNS servers for the Pod.
There can be at most 3 IP addresses specified. When the Pod’s dnsPolicy is set to “None”,
the list must contain at least one IP address, otherwise this property is optional.
The servers listed will be combined to the base nameservers generated from the 
specified DNS policy with duplicate addresses removed.
searches: a list of DNS search domains for hostname lookup in the Pod.
This property is optional. When specified, the provided list will be merged into 
the base search domain names generated from the chosen DNS policy. Duplicate domain names are removed.
Kubernetes allows for at most 6 search domains.
options: an optional list of objects where each object may have a name property
(required) and a value property (optional). The contents in this property will be merged
to the options generated from the specified DNS policy. Duplicate entries are removed.
The following is an example Pod with custom DNS settings:

service/networking/custom-dns.yaml Copy service/networking/custom-dns.yaml to clipboard
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 1.2.3.4
    searches:
      - ns1.svc.cluster-domain.example
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0
When the Pod above is created, the container test gets the following contents in its /etc/resolv.conf file:

nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
For IPv6 setup, search path and name server should be setup like this:

kubectl exec -it dns-example -- cat /etc/resolv.conf
The output is similar to this:

nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-



---

Debugging DNS Resolution

https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/



1. Write a manifest for simple Pod to use as a test environment

Create a file named busybox.yaml with the following contents:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always


Then create a pod using this file and verify its status:


kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml

Once that pod is running, you can exec nslookup in that environment. If you see something like the 

1. Write command to check dns resolution
   kubectl exec -ti busybox -- nslookup kubernetes.default


2. Write command to Check the local DNS configuration first

kubectl exec busybox cat /etc/resolv.conf

search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5

Errors such as the following indicate a problem with the coredns/kube-dns add-on or associated Services:

kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'


kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'


Check if the DNS pod is running

kubectl get pods --namespace=kube-system -l k8s-app=kube-dns

Check for Errors in the DNS pod
Use kubectl logs command to see logs for the DNS containers.

For CoreDNS:

for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl 
logs --namespace=kube-system $p; done

Here is an example of a healthy CoreDNS log:

.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c


For kube-dns, there are 3 sets of logs:

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system 
-l k8s-app=kube-dns -o name | head -1) -c kubedns

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system
-l k8s-app=kube-dns -o name | head -1) -c dnsmasq

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system
-l k8s-app=kube-dns -o name | head -1) -c sidecar


Is DNS service up?
Verify that the DNS service is up by using the kubectl get service command.

kubectl get svc --namespace=kube-system
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      <none>        53/UDP,53/TCP        1h
...
Note that the service name will be “kube-dns” for both CoreDNS and 
kube-dns deployments. If you have created the service or in the case
it should be created by default but it does not appear, see debugging services for more information.



Are DNS endpoints exposed?
You can verify that DNS endpoints are exposed by using the kubectl get endpoints command.

kubectl get ep kube-dns --namespace=kube-system
NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
If you do not see the endpoints, see endpoints section in the debugging services documentation.

For additional Kubernetes DNS examples, see the cluster-dns examples in the Kubernetes GitHub repository.


Are DNS queries being received/processed?
You can verify if queries are being received by CoreDNS by adding the 
log plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile
is held in a ConfigMap named coredns. To edit it, use the command …

kubectl -n kube-system edit configmap coredns
Then add log in the Corefile section per the example below.

apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        log
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }

After saving the changes, it may take up to minute or two for Kubernetes
to propagate these changes to the CoreDNS pods.

Next, make some queries and view the logs per the sections above in this document. 
If CoreDNS pods are receiving the queries, you should see them in the logs.

Here is an example of a query in the log.

.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 "A IN kubernetes.default.svc.cluster.local.
udp 54 false 512" NOERROR qr,aa,rd,ra 106 0.000066649s


Known issues
Some Linux distributions (e.g. Ubuntu), use a local DNS resolver by default (systemd-resolved). 
Systemd-resolved moves and replaces /etc/resolv.conf with a stub file that can cause a 
fatal forwarding loop when resolving names in upstream servers. This can be fixed manually by 
using kubelet’s --resolv-conf flag to point to the correct resolv.conf (With systemd-resolved, 
this is /run/systemd/resolve/resolv.conf). kubeadm (>= 1.11) automatically detects systemd-resolved,
and adjusts the kubelet flags accordingly.



Linux’s libc is impossibly stuck (see this bug from 2005) with limits of just 3 DNS 
nameserver records and 6 DNS search records. Kubernetes needs to consume 1 nameserver
record and 3 search records. This means that if a local installation already uses 3 
nameservers or uses more than 3 searches, some of those settings will be lost.
As a partial workaround, the node can run dnsmasq which will provide more nameserver entries, 
but not more search entries. You can also use kubelet’s --resolv-conf flag.

If you are using Alpine version 3.3 or earlier as your base image, DNS may not work properly
owing to a known issue with Alpine. Check here for more information.


---

Customizing DNS Service

https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/

CoreDNS

CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS, 
complying with the dns specifications.

CoreDNS ConfigMap options
CoreDNS is a DNS server that is modular and pluggable, and each plugin adds new functionality
to CoreDNS. This can be configured by maintaining a Corefile, which is the CoreDNS configuration file.
A cluster administrator can modify the ConfigMap for the CoreDNS Corefile to change how service discovery works.

In Kubernetes, CoreDNS is installed with the following default Corefile configuration.

apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
The Corefile configuration includes the following plugins of CoreDNS:

errors: Errors are logged to stdout.
health: Health of CoreDNS is reported to http://localhost:8080/health.
kubernetes: CoreDNS will reply to DNS queries based on IP of the services and pods of Kubernetes. 
You can find more details here.
The pods insecure option is provided for backward compatibility with kube-dns. 
You can use the pods verified option, which returns an A record only if there exists 
a pod in same namespace with matching IP. The pods disabled option can be used if you don’t use pod records.

prometheus: Metrics of CoreDNS are available at http://localhost:9153/metrics in Prometheus format.
forward: Any queries that are not within the cluster domain of Kubernetes will be forwarded
to predefined resolvers (/etc/resolv.conf).
cache: This enables a frontend cache.
loop: Detects simple forwarding loops and halts the CoreDNS process if a loop is found.
reload: Allows automatic reload of a changed Corefile. After you edit the ConfigMap configuration,
allow two minutes for your changes to take effect.
loadbalance: This is a round-robin DNS loadbalancer that randomizes the order of A, AAAA, and MX records in the answer.
You can modify the default CoreDNS behavior by modifying the ConfigMap.



Configuration of Stub-domain and upstream nameserver using CoreDNS
CoreDNS has the ability to configure stubdomains and upstream nameservers using the forward plugin.

Example
If a cluster operator has a Consul domain server located at 10.150.0.1, and all Consul names have
the suffix .consul.local. To configure it in CoreDNS, the cluster administrator creates the 
following stanza in the CoreDNS ConfigMap.

consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
To explicitly force all non-cluster DNS lookups to go through a specific nameserver
at 172.16.0.1, point the forward to the nameserver instead of /etc/resolv.conf

forward .  172.16.0.1
The final ConfigMap along with the default Corefile configuration looks like:

apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . 172.16.0.1
        cache 30
        loop
        reload
        loadbalance
    }
    consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
In Kubernetes version 1.10 and later, kubeadm supports automatic translation of the 
CoreDNS ConfigMap from the kube-dns ConfigMap. Note: While kube-dns accepts an FQDN 
for stubdomain and nameserver (eg: ns.foo.com), CoreDNS does not support this feature. 
During translation, all FQDN nameservers will be omitted from the CoreDNS config

CoreDNS configuration equivalent to kube-dns
CoreDNS supports the features of kube-dns and more. A ConfigMap created for
kube-dns to support StubDomainsand upstreamNameservers translates to the forward plugin
\in CoreDNS. Similarly, the Federations plugin in kube-dns translates to the federation plugin in CoreDNS.

Example
This example ConfigMap for kubedns specifies federations, stubdomains and upstreamnameservers:

apiVersion: v1
data:
  federations: |
    {"foo" : "foo.feddomain.com"}
  stubDomains: |
    {"abc.com" : ["1.2.3.4"], "my.cluster.local" : ["2.3.4.5"]}
  upstreamNameservers: |
    ["8.8.8.8", "8.8.4.4"]
kind: ConfigMap
The equivalent configuration in CoreDNS creates a Corefile:

For federations:

federation cluster.local {
       foo foo.feddomain.com
    }
For stubDomains:

abc.com:53 {
    errors
    cache 30
    forward . 1.2.3.4
}
my.cluster.local:53 {
    errors
    cache 30
    forward . 2.3.4.5
}
The complete Corefile with the default plugins:

.:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        federation cluster.local {
           foo foo.feddomain.com
        }
        prometheus :9153
        forward .  8.8.8.8 8.8.4.4
        cache 30
    }
    abc.com:53 {
        errors
        cache 30
        forward . 1.2.3.4
    }
    my.cluster.local:53 {
        errors
        cache 30
        forward . 2.3.4.5
    }


Kube-dns
Kube-dns is now available as an optional DNS server since CoreDNS is now the default.
The running DNS Pod holds 3 containers:

“kubedns“: watches the Kubernetes master for changes in Services and Endpoints, and maintains
in-memory lookup structures to serve DNS requests.
“dnsmasq“: adds DNS caching to improve performance.
“sidecar“: provides a single health check endpoint to perform healthchecks for dnsmasq and kubedns.
Configure stub-domain and upstream DNS servers
Cluster administrators can specify custom stub domains and upstream nameservers by providing
a ConfigMap for kube-dns (kube-system:kube-dns).

For example, the following ConfigMap sets up a DNS configuration with a single stub domain and two upstream nameservers:

apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"acme.local": ["1.2.3.4"]}
  upstreamNameservers: |
    ["8.8.8.8", "8.8.4.4"]
DNS requests with the “.acme.local” suffix are forwarded to a DNS listening at
1.2.3.4. Google Public DNS serves the upstream queries.

The table below describes how queries with certain domain names map to their destination DNS servers:


---
Kubernetes DNS-Based Service Discovery

https://linuxacademy.com/cp/courses/lesson/course/4018/lesson/5/module/327
https://github.com/kubernetes/dns/blob/master/docs/specification.md


This document is a specification for DNS-based Kubernetes service discovery.
While service discovery in Kubernetes may be provided via other protocols and mechanisms,
DNS is very commonly used and is a highly recommended add-on. The actual DNS service itself
need not be provided by the default Kube-DNS implementation. This document is intended to provide
a baseline for commonality between implementations







---

Autoscale the DNS Service in a Cluster

https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/

Determining whether DNS horizontal autoscaling is already enabled

kubectl get deployment --namespace=kube-system


https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#tuning-autoscaling-parameters

Tuning autoscaling parameters

Verify that the dns-autoscaler ConfigMap exists:

kubectl get configmap --namespace=kube-system
The output is similar to this:

NAME                  DATA      AGE
...
dns-autoscaler        1         ...
...
Modify the data in the ConfigMap:

kubectl edit configmap dns-autoscaler --namespace=kube-system


Look for this line:

linear: '{"coresPerReplica":256,"min":1,"nodesPerReplica":16}'
Modify the fields according to your needs. The “min” field indicates the minimal number of DNS backends. 
The actual number of backends number is calculated using this equation:

replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )


Note that the values of both coresPerReplica and nodesPerReplica are integers.

The idea is that when a cluster is using nodes that have many cores, coresPerReplica dominates.
When a cluster is using nodes that have fewer cores, nodesPerReplica dominates.

There are other supported scaling patterns

Disable DNS horizontal autoscaling

There are a few options for tuning DNS horizontal autoscaling. Which option to use depends on different conditions.

Option 1: Scale down the dns-autoscaler deployment to 0 replicas
This option works for all situations. Enter this command:

kubectl scale deployment --replicas=0 dns-autoscaler --namespace=kube-system

Verify that the replica count is zero:

kubectl get deployment --namespace=kube-system

Option 2: Delete the dns-autoscaler deployment
This option works if dns-autoscaler is under your own control, which means no one will re-create it:

kubectl delete deployment dns-autoscaler --namespace=kube-system

Option 3: Delete the dns-autoscaler manifest file from the master node
This option works if dns-autoscaler is under control of the (deprecated) Addon Manager,
and you have write access to the master node.

Sign in to the master node and delete the corresponding manifest file. The common path for this dns-autoscaler is:

/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml


Understanding how DNS horizontal autoscaling works
The cluster-proportional-autoscaler application is deployed separately from the DNS service.

An autoscaler Pod runs a client that polls the Kubernetes API server for the number of nodes
and cores in the cluster.

A desired replica count is calculated and applied to the DNS backends based on the current 
schedulable nodes and cores and the given scaling parameters.

The scaling parameters and data points are provided via a ConfigMap to the autoscaler,
and it refreshes its parameters table every poll interval to be up to date with the latest desired scaling parameters.

Changes to the scaling parameters are allowed without rebuilding or restarting the autoscaler Pod.

The autoscaler provides a controller interface to support two control patterns: linear and ladder.



Enabling DNS horizontal autoscaling

In this section, you create a Deployment. The Pods in the Deployment run a container 
based on the cluster-proportional-autoscaler-amd64 image.

Create a file named dns-horizontal-autoscaler.yaml with this content:

admin/dns/dns-horizontal-autoscaler.yaml Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-autoscaler
  namespace: kube-system
  labels:
    k8s-app: dns-autoscaler
spec:
  selector:
    matchLabels:
      k8s-app: dns-autoscaler
  template:
    metadata:
      labels:
        k8s-app: dns-autoscaler
    spec:
      containers:
      - name: autoscaler
        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0
        resources:
          requests:
            cpu: 20m
            memory: 10Mi
        command:
        - /cluster-proportional-autoscaler
        - --namespace=kube-system
        - --configmap=dns-autoscaler
        - --target=<SCALE_TARGET>
        # When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.
        # If using small nodes, "nodesPerReplica" should dominate.
        - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":1}}
        - --logtostderr=true
        - --v=2
In the file, replace <SCALE_TARGET> with your scale target.

Go to the directory that contains your configuration file, and enter this command to create the Deployment:

kubectl apply -f dns-horizontal-autoscaler.yaml
The output of a successful command is:

deployment.apps/kube-dns-autoscaler created


DNS horizontal autoscaling is now enabled.


---
LAB
Creating a Service and Discovering DNS Names in Kubernetes


Additional Information and Resources
You have been given a three-node cluster. Within that cluster, you must
perform the following tasks in order to create a service and resolve the DNS names
for that service. You will create the necessary Kubernetes resources in order to perform this DNS query.
To adequately complete this hands-on lab, you must have a working deployment, a working service, 
and be able to record the DNS name of the service within your Kubernetes cluster. 
This means you will perform the following tasks:

Create an nginx deployment using the latest nginx image.
Verify the deployment has been created successfully.
Create a service from the nginx deployment created in the previous objective.
Verify the service has been created successfully.
Create a pod that will allow you to perform the DNS query.
Verify that the pod has been created successfully.
Perform the DNS query to the service created in an earlier objective.
Record the DNS name of the service.



Create an nginx deployment, and verify it was successful.

kubectl run nginx --image=nginx

Use this command to verify deployment was successful:

kubectl get deployments

Create a service, and verify the service was successful.

kubectl expose deployment nginx --port 80 --type NodePort

Use this command to verify the service was created:

kubectl get services

Create a pod that will allow you to perform the DNS query.

apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    name: busybox
  restartPolicy: Always


Perform a DNS query to the service


kubectl exec busybox -- nslookup nginx

Perform a DNS query to the service.

<service-name>;.default.svc.cluster.local







































































































































































