---
Running Multiple Schedulers for Multiple Pods

https://linuxacademy.com/cp/courses/lesson/course/4022/lesson/2/module/327

In Kubernetes, you can run multiple schedulers simultaneously. You can then use different 
schedulers to schedule different pods. You may, for example, want to set different 
rules for the scheduler to run all of your pods on one node. In this lesson, I will show you 
how to deploy a new scheduler alongside your default scheduler and then schedule three 
different pods using the two schedulers.


ClusterRole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: csinodes-admin
rules:
- apiGroups: ["storage.k8s.io"]
  resources: ["csinodes"]
  verbs: ["get", "watch", "list"]

  
  
  
  
  
  
ClusterRoleBinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-csinodes-global
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: csinodes-admin
  apiGroup: rbac.authorization.k8s.io
  
  
Role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: system:serviceaccount:kube-system:my-scheduler
  namespace: kube-system
rules:
- apiGroups:
  - storage.k8s.io
  resources:
  - csinodes
  verbs:
  - get
  - list
  - watch
RoleBinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-csinodes
  namespace: kube-system
subjects:
- kind: User
  name: kubernetes-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role 
  name: system:serviceaccount:kube-system:my-scheduler
  apiGroup: rbac.authorization.k8s.io
Edit the existing kube-scheduler cluster role with kubectl edit clusterrole system:kube-scheduler 
and add the following:

- apiGroups:
  - ""
  resourceNames:
  - kube-scheduler
  - my-scheduler
  resources:
  - endpoints
  verbs:
  - delete
  - get
  - patch
  - update
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - watch
  - list
  - get
My-scheduler.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: my-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=false
        - --scheduler-name=my-scheduler
        image: chadmcrowell/custom-scheduler
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
        name: kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10251
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts: []
      hostNetwork: false
      hostPID: false
      volumes: []
Run the deployment for my-scheduler:

kubectl create -f my-scheduler.yaml
View your new scheduler in the kube-system namespace:

kubectl get pods -n kube-system
pod1.yaml

apiVersion: v1
kind: Pod
metadata:
  name: no-annotation
  labels:
    name: multischeduler-example
spec:
  containers:
  - name: pod-with-no-annotation-container
    image: k8s.gcr.io/pause:2.0
pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: annotation-default-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: default-scheduler
  containers:
  - name: pod-with-default-annotation-container
    image: k8s.gcr.io/pause:2.0
pod3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: annotation-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: my-scheduler
  containers:
  - name: pod-with-second-annotation-container
    image: k8s.gcr.io/pause:2.0
View the pods as they are created:

kubectl get pods -o wide

---

Configure Multiple Schedulers

https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/

Kubernetes ships with a default scheduler that is described here. If the default scheduler 
does not suit your needs you can implement your own scheduler. Not just that, you can even run 
multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes 
what scheduler to use for each of your pods. Let’s learn how to run multiple schedulers in 
Kubernetes with an example.

A detailed description of how to implement a scheduler is outside the scope of this document. 
Please refer to the kube-scheduler implementation in pkg/scheduler in the Kubernetes source 
directory for a canonical example.


Package the scheduler
Define a Kubernetes Deployment for the scheduler
Run the second scheduler in the cluster
Specify schedulers for pods


Package the scheduler
Package your scheduler binary into a container image. For the purposes of this example, let’s just use
the default scheduler (kube-scheduler) as our second scheduler as well. Clone the Kubernetes source code
from GitHub and build the source.

git clone https://github.com/kubernetes/kubernetes.git
cd kubernetes
make
Create a container image containing the kube-scheduler binary. Here is the Dockerfile to build the image:

FROM busybox
ADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler
Save the file as Dockerfile, build the image and push it to a registry. This example pushes
the image to Google Container Registry (GCR). For more details, please read the GCR documentation.

docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .
gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0



Define a Kubernetes Deployment for the scheduler

Now that we have our scheduler in a container image, we can just create a pod 
config for it and run it in our Kubernetes cluster. But instead of creating a 
pod directly in the cluster, let’s use a Deployment for this example. A Deployment manages a 
Replica Set which in turn manages the pods, thereby making the scheduler resilient to failures. 
Here is the deployment config. Save it as my-scheduler.yaml


apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: my-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=false
        - --scheduler-name=my-scheduler
        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
        name: kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10251
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts: []
      hostNetwork: false
      hostPID: false
      volumes: []
An important thing to note here is that the name of the scheduler specified as an argument to the scheduler 
command in the container spec should be unique. This is the name that is matched against the 
value of the optional spec.schedulerName on pods, to determine whether this scheduler is responsible 
for scheduling a particular pod.

Note also that we created a dedicated service account my-scheduler and bind the 
cluster role system:kube-scheduler to it so that it can acquire the same privileges as kube-scheduler


Run the second scheduler in the cluster


---

LAB: Scheduling Pods with Taints and Tolerations in Kubernetes

https://app.linuxacademy.com/hands-on-labs/d4d79e45-f799-42f9-b136-cbb67c5ee553?
redirect_uri=https:%2F%2Flinuxacademy.com%2Fcp%2Fmodules%2Fview%2Fid%2F327

Additional Information and Resources
You have been given a three-node cluster. Within that cluster, you must perform
the following tasks to taint the production node in order to repel work. You will 
create the necessary taint to properly label one of the nodes “prod.” Then you will 
deploy two pods — one to each environment. One pod spec will contain the toleration 
for the taint. You must perform the following tasks in order to complete this hands-on lab:

Taint one of the worker nodes to identify the prod environment.
Create the YAML spec for a pod that will be scheduled to the dev environment.
Create the YAML spec for a pod that will be scheduled to the prod environment.
Deploy each pod to their respective environments.
Verify each pod has been scheduled successfully to each environment.


Taint one of the worker nodes to repel work.

Use the following command to taint the node:

kubectl taint node <node_name> node-type=prod:NoSchedule

Allow a pod to be scheduled to the prod environment.


Use the following YAML to create a deployment and a pod that will tolerate the prod environment:

apiVersion: apps/v1
kind: Deployment
metadata:
 name: prod
spec:
 replicas: 1
 selector:
   matchLabels:
     app: prod
 template:
   metadata:
     labels:
       app: prod
   spec:
     containers:
     - args:
       - sleep
       - "3600"
       image: busybox
       name: main
     tolerations:
     - key: node-type
       operator: Equal
       value: prod
       effect: NoSchedule



Verify each pod has been scheduled and verify the toleration.

Use the following command to verify the pods have been scheduled:

kubectl get pods -o wide
Verify the toleration of the production pod:

kubectl get pods <pod_name> -o yaml



Node Affinity ensures that pods are hosted on particular nodes
apiVersion: v1
kind: Pod
metadata:
name: with-node-affinity
spec:
affinity:
nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: Kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
Pod Affinity ensures two pods to be co-located in a single node.

apiVersion: v1
kind: Pod
metadata:
name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1

The pod affinity rule says that the pod can be scheduled to a node only if that node is in the same zone as at least
one already-running pod that has a label with key “security” and value “S1”

Deploying the Cache Pod
It’s time to deploy the Redis Pod that acts as the cache layer. We want to make sure that no 
two Redis Pods run on the same node. For that, we will define an anti-affinity rule.


https://thenewstack.io/implement-node-and-pod-affinity-anti-affinity-in-kubernetes-a-practical-example/






































  
  
