---
Running Multiple Schedulers for Multiple Pods

https://linuxacademy.com/cp/courses/lesson/course/4022/lesson/2/module/327

In Kubernetes, you can run multiple schedulers simultaneously. You can then use different 
schedulers to schedule different pods. You may, for example, want to set different 
rules for the scheduler to run all of your pods on one node. In this lesson, I will show you 
how to deploy a new scheduler alongside your default scheduler and then schedule three 
different pods using the two schedulers.


ClusterRole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: csinodes-admin
rules:
- apiGroups: ["storage.k8s.io"]
  resources: ["csinodes"]
  verbs: ["get", "watch", "list"]

  
  
  
  
  
  
ClusterRoleBinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-csinodes-global
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: csinodes-admin
  apiGroup: rbac.authorization.k8s.io
  
  
Role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: system:serviceaccount:kube-system:my-scheduler
  namespace: kube-system
rules:
- apiGroups:
  - storage.k8s.io
  resources:
  - csinodes
  verbs:
  - get
  - list
  - watch
RoleBinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-csinodes
  namespace: kube-system
subjects:
- kind: User
  name: kubernetes-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role 
  name: system:serviceaccount:kube-system:my-scheduler
  apiGroup: rbac.authorization.k8s.io
Edit the existing kube-scheduler cluster role with kubectl edit clusterrole system:kube-scheduler 
and add the following:

- apiGroups:
  - ""
  resourceNames:
  - kube-scheduler
  - my-scheduler
  resources:
  - endpoints
  verbs:
  - delete
  - get
  - patch
  - update
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - watch
  - list
  - get
My-scheduler.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: my-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=false
        - --scheduler-name=my-scheduler
        image: chadmcrowell/custom-scheduler
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
        name: kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10251
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts: []
      hostNetwork: false
      hostPID: false
      volumes: []
Run the deployment for my-scheduler:

kubectl create -f my-scheduler.yaml
View your new scheduler in the kube-system namespace:

kubectl get pods -n kube-system
pod1.yaml

apiVersion: v1
kind: Pod
metadata:
  name: no-annotation
  labels:
    name: multischeduler-example
spec:
  containers:
  - name: pod-with-no-annotation-container
    image: k8s.gcr.io/pause:2.0
pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: annotation-default-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: default-scheduler
  containers:
  - name: pod-with-default-annotation-container
    image: k8s.gcr.io/pause:2.0
pod3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: annotation-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: my-scheduler
  containers:
  - name: pod-with-second-annotation-container
    image: k8s.gcr.io/pause:2.0
View the pods as they are created:

kubectl get pods -o wide

---

Configure Multiple Schedulers

https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/

Kubernetes ships with a default scheduler that is described here. If the default scheduler 
does not suit your needs you can implement your own scheduler. Not just that, you can even run 
multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes 
what scheduler to use for each of your pods. Let’s learn how to run multiple schedulers in 
Kubernetes with an example.

A detailed description of how to implement a scheduler is outside the scope of this document. 
Please refer to the kube-scheduler implementation in pkg/scheduler in the Kubernetes source 
directory for a canonical example.


Package the scheduler
Define a Kubernetes Deployment for the scheduler
Run the second scheduler in the cluster
Specify schedulers for pods


Package the scheduler
Package your scheduler binary into a container image. For the purposes of this example, let’s just use
the default scheduler (kube-scheduler) as our second scheduler as well. Clone the Kubernetes source code
from GitHub and build the source.

git clone https://github.com/kubernetes/kubernetes.git
cd kubernetes
make
Create a container image containing the kube-scheduler binary. Here is the Dockerfile to build the image:

FROM busybox
ADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler
Save the file as Dockerfile, build the image and push it to a registry. This example pushes
the image to Google Container Registry (GCR). For more details, please read the GCR documentation.

docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .
gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0



Define a Kubernetes Deployment for the scheduler

Now that we have our scheduler in a container image, we can just create a pod 
config for it and run it in our Kubernetes cluster. But instead of creating a 
pod directly in the cluster, let’s use a Deployment for this example. A Deployment manages a 
Replica Set which in turn manages the pods, thereby making the scheduler resilient to failures. 
Here is the deployment config. Save it as my-scheduler.yaml


apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: my-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=false
        - --scheduler-name=my-scheduler
        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
        name: kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10251
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts: []
      hostNetwork: false
      hostPID: false
      volumes: []
An important thing to note here is that the name of the scheduler specified as an argument to the scheduler 
command in the container spec should be unique. This is the name that is matched against the 
value of the optional spec.schedulerName on pods, to determine whether this scheduler is responsible 
for scheduling a particular pod.

Note also that we created a dedicated service account my-scheduler and bind the 
cluster role system:kube-scheduler to it so that it can acquire the same privileges as kube-scheduler


Run the second scheduler in the cluster


---

LAB: Scheduling Pods with Taints and Tolerations in Kubernetes

https://app.linuxacademy.com/hands-on-labs/d4d79e45-f799-42f9-b136-cbb67c5ee553?
redirect_uri=https:%2F%2Flinuxacademy.com%2Fcp%2Fmodules%2Fview%2Fid%2F327

Additional Information and Resources
You have been given a three-node cluster. Within that cluster, you must perform
the following tasks to taint the production node in order to repel work. You will 
create the necessary taint to properly label one of the nodes “prod.” Then you will 
deploy two pods — one to each environment. One pod spec will contain the toleration 
for the taint. You must perform the following tasks in order to complete this hands-on lab:

Taint one of the worker nodes to identify the prod environment.
Create the YAML spec for a pod that will be scheduled to the dev environment.
Create the YAML spec for a pod that will be scheduled to the prod environment.
Deploy each pod to their respective environments.
Verify each pod has been scheduled successfully to each environment.


Taint one of the worker nodes to repel work.

Use the following command to taint the node:

kubectl taint node <node_name> node-type=prod:NoSchedule

Allow a pod to be scheduled to the prod environment.


Use the following YAML to create a deployment and a pod that will tolerate the prod environment:

apiVersion: apps/v1
kind: Deployment
metadata:
 name: prod
spec:
 replicas: 1
 selector:
   matchLabels:
     app: prod
 template:
   metadata:
     labels:
       app: prod
   spec:
     containers:
     - args:
       - sleep
       - "3600"
       image: busybox
       name: main
     tolerations:
     - key: node-type
       operator: Equal
       value: prod
       effect: NoSchedule



Verify each pod has been scheduled and verify the toleration.

Use the following command to verify the pods have been scheduled:

kubectl get pods -o wide
Verify the toleration of the production pod:

kubectl get pods <pod_name> -o yaml



Node Affinity ensures that pods are hosted on particular nodes
apiVersion: v1
kind: Pod
metadata:
name: with-node-affinity
spec:
affinity:
nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: Kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
Pod Affinity ensures two pods to be co-located in a single node.

apiVersion: v1
kind: Pod
metadata:
name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1

The pod affinity rule says that the pod can be scheduled to a node only if that node is in the same zone as at least
one already-running pod that has a label with key “security” and value “S1”

Deploying the Cache Pod
It’s time to deploy the Redis Pod that acts as the cache layer. We want to make sure that no 
two Redis Pods run on the same node. For that, we will define an anti-affinity rule.


https://thenewstack.io/implement-node-and-pod-affinity-anti-affinity-in-kubernetes-a-practical-example/

nodeSelector
nodeSelector is the simplest recommended form of node selection constraint. 
nodeSelector is a field of PodSpec. It specifies a map of key-value pairs. 
For the pod to be eligible to run on a node, the node must have 
each of the indicated key-value pairs as labels (it can have additional labels as well). 

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
    
    
    
 Concepts
Overview
What is Kubernetes?
Kubernetes Components
The Kubernetes API
Working with Kubernetes Objects
Understanding Kubernetes Objects
Kubernetes Object Management
Object Names and IDs
Namespaces
Labels and Selectors
Annotations
Field Selectors
Recommended Labels
Cluster Architecture
Nodes
Control Plane-Node Communication
Controllers
Cloud Controller Manager
Containers
Containers overview
Images
Container Environment
Runtime Class
Container Lifecycle Hooks
Workloads
Pods
Pod Overview
Pods
Pod Lifecycle
Init Containers
Pod Preset
Pod Topology Spread Constraints
Disruptions
Ephemeral Containers
Controllers
ReplicaSet
ReplicationController
Deployments
StatefulSets
DaemonSet
Garbage Collection
TTL Controller for Finished Resources
Jobs - Run to Completion
CronJob
Services, Load Balancing, and Networking
Service
Service Topology
EndpointSlices
DNS for Services and Pods
Connecting Applications with Services
Ingress
Ingress Controllers
Network Policies
Adding entries to Pod /etc/hosts with HostAliases
IPv4/IPv6 dual-stack
Storage
Volumes
Persistent Volumes
Volume Snapshots
CSI Volume Cloning
Storage Classes
Volume Snapshot Classes
Dynamic Volume Provisioning
Node-specific Volume Limits
Configuration
Configuration Best Practices
ConfigMaps
Secrets
Managing Resources for Containers
Pod Overhead
Resource Bin Packing for Extended Resources
Organizing Cluster Access Using kubeconfig Files
Pod Priority and Preemption
Security
Overview of Cloud Native Security
Pod Security Standards
Policies
Limit Ranges
Resource Quotas
Pod Security Policies
Scheduling and Eviction
Kubernetes Scheduler
Taints and Tolerations
Assigning Pods to Nodes
Scheduling Framework
Scheduler Performance Tuning
Cluster Administration
Cluster Administration Overview
Certificates
Cloud Providers
Managing Resources
Cluster Networking
Logging Architecture
Metrics For The Kubernetes Control Plane
Configuring kubelet Garbage Collection
Proxies in Kubernetes
API Priority and Fairness
Installing Addons
Extending Kubernetes
Extending your Kubernetes Cluster
Extending the Kubernetes API
Custom Resources
Extending the Kubernetes API with the aggregation layer
Compute, Storage, and Networking Extensions
Network Plugins
Device Plugins
Operator pattern
Service Catalog
Poseidon-Firmament Scheduler
Edit This Page

Assigning Pods to Nodes
You can constrain a Pod to only be able to run on particular Node(s), or to prefer to run on particular nodes. There are several ways to do this, and the recommended approaches all use label selectors to make the selection. Generally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement (e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.) but there are some circumstances where you may want more control on a node where a pod lands, for example to ensure that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different services that communicate a lot into the same availability zone.

nodeSelector
Interlude: built-in node labels
Node isolation/restriction
Affinity and anti-affinity
nodeName
What's next
nodeSelector
nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). The most common usage is one key-value pair.

Let’s walk through an example of how to use nodeSelector.

Step Zero: Prerequisites
This example assumes that you have a basic understanding of Kubernetes pods and that you have set up a Kubernetes cluster.

Step One: Attach label to the node
Run kubectl get nodes to get the names of your cluster’s nodes. Pick out the one that you want to add a label to, and then run kubectl label nodes <node-name> <label-key>=<label-value> to add a label to the node you’ve chosen. For example, if my node name is ‘kubernetes-foo-node-1.c.a-robinson.internal’ and my desired label is ‘disktype=ssd’, then I can run kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd.

You can verify that it worked by re-running kubectl get nodes --show-labels and checking that the node now has a label. You can also use kubectl describe node "nodename" to see the full list of labels of the given node.

Step Two: Add a nodeSelector field to your pod configuration
Take whatever pod config file you want to run, and add a nodeSelector section to it, like this. For example, if this is my pod config:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
Then add a nodeSelector like so:



In addition to labels you attach, nodes come pre-populated with a standard set of labels. These labels are

kubernetes.io/hostname
failure-domain.beta.kubernetes.io/zone
failure-domain.beta.kubernetes.io/region
topology.kubernetes.io/zone
topology.kubernetes.io/region
beta.kubernetes.io/instance-type
node.kubernetes.io/instance-type
kubernetes.io/os
kubernetes.io/arch   


Affinity and anti-affinity

nodeSelector provides a very simple way to constrain pods to nodes with particular labels.
The affinity/anti-affinity feature, greatly expands the types of constraints you can express.
The key enhancements are

The affinity/anti-affinity language is more expressive. The language offers more matching rules
besides exact matches created with a logical AND operation;

you can constrain against labels on other pods running on the node (or other topological domain), rather than against
labels on the node itself, which allows rules about which pods can and cannot be co-located

The affinity feature consists of two types of affinity, “node affinity” and “inter-pod affinity/anti-affinity”.

Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while 
inter-pod affinity/anti-affinity constrains against pod labels rather than node labels,

Node affinity

Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes 
your pod is eligible to be scheduled on, based on labels on the node

There are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution
and preferredDuringSchedulingIgnoredDuringExecution

Thus an example of requiredDuringSchedulingIgnoredDuringExecution 
would be “only run the pod on nodes with Intel CPUs” and an example 
preferredDuringSchedulingIgnoredDuringExecution would be 
“try to run this set of pods in failure zone XYZ, but if it’s not possible, then allow some to run elsewhere”.

Node affinity is specified as field nodeAffinity of field affinity in the PodSpec

apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        #If you specify multiple nodeSelectorTerms associated with nodeAffinity types, 
        #then the pod can be scheduled onto a node if one of the nodeSelectorTerms can be satisfied.
        - matchExpressions:
        #If you specify multiple matchExpressions associated with nodeSelectorTerms,
        #then the pod can be scheduled onto a node only if all matchExpressions is satisfied
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
      #is in the range 1-100
      #the scheduler will compute a sum by iterating through the elements of this
      #field and adding “weight” to the sum if the node matches the corresponding MatchExpressions.
      #This score is then combined with the scores of 
      #other priority functions for the node. The node(s) with the highest total score are the most preferred
      
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0
    
    
 This node affinity rule says the pod can only be placed on a node with a label whose key
 is kubernetes.io/e2e-az-name and whose value is either e2e-az1 or e2e-az2. In addition, 
 among nodes that meet that criteria, nodes with a label whose key 
 is another-node-label-key and whose value is another-node-label-value should be preferred.   
 
 
 Inter-pod affinity and anti-affinity
 
 Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod 
 is eligible to be scheduled based on labels
 on pods that are already running on the node rather than based on labels on nodes.

Inter-pod affinity is specified as field podAffinity of field affinity in the PodSpec
inter-pod anti-affinity is specified as field podAntiAffinity of field affinity in the PodSpec


apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: k8s.gcr.io/pause:2.0


The affinity on this pod defines one pod affinity rule and one pod anti-affinity rule
In this example, the podAffinity is requiredDuringSchedulingIgnoredDuringExecution while
the podAntiAffinity is preferredDuringSchedulingIgnoredDuringExecution

The pod affinity rule says that the pod can be scheduled onto a node only if that node is in the same zone
as at least one already-running pod that has a label with key “security” and value “S1”

The pod anti-affinity rule says that the pod prefers not to be scheduled onto a node if that
node is already running a pod with label having key “security” and value “S2”.


More Practical Use-cases
Interpod Affinity and AntiAffinity can be even more useful when they are
used with higher level collections such as ReplicaSets, StatefulSets, 
Deployments, etc. One can easily configure that a set of workloads 
should be co-located in the same defined topology, eg., the same node.






























  
  
