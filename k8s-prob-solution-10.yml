nodes can experience failures such as crashes, network disruption or storage faults that makes it unavailable or puts it in 'Not Ready' 
condition. But how does the Controller knows about it ❓ 
The chronology of events follow as below :
👉 Kubelet provides various operational status updates and heartbeat information about the nodes 
👉 It uses the Lease mechanism (k8s ver > 1.17) to report heartbeat  that uses 𝒓𝒆𝒏𝒆𝒘𝑻𝒊𝒎𝒆 and 𝒉𝒐𝒍𝒅𝒆𝒓𝑰𝒅𝒆𝒏𝒕𝒊𝒕𝒚 to represent the last time each node was updated
 👉 Kubelet and Controller work asynchronously, one reporting and the other monitoring.
 👉 𝒏𝒐𝒅𝒆𝑳𝒆𝒂𝒔𝒆𝑫𝒖𝒓𝒂𝒕𝒊𝒐𝒏𝑺𝒆𝒄𝒐𝒏𝒅𝒔 on kubelet determines how often the Lease object is updated; the calculated value times 0.25 is the actual seconds
 👉 𝒏𝒐𝒅𝒆-𝒎𝒐𝒏𝒊𝒕𝒐𝒓-𝒑𝒆𝒓𝒊𝒐𝒅 and 𝒏𝒐𝒅𝒆-𝒎𝒐𝒏𝒊𝒕𝒐𝒓-𝒈𝒓𝒂𝒄𝒆-𝒑𝒆𝒓𝒊𝒐𝒅 on the Controller determine how often the Controller checks and how long until it's considered "NotReady"
👉 By using a goroutine, the 𝘮𝘰𝘯𝘪𝘵𝘰𝘳𝘕𝘰𝘥𝘦𝘏𝘦𝘢𝘭𝘵𝘩 function is executed every 𝒏𝒐𝒅𝒆-𝒎𝒐𝒏𝒊𝒕𝒐𝒓-𝒑𝒆𝒓𝒊𝒐𝒅 second to check the node's status
👉 By default, it takes at least 40 seconds to detect a node failure while a pod can survive on a faulty node for 300 seconds!

hashtag#k8s utilizes the Taint/Toleration mechanism to achieve automated rescheduling of the pods to 'healthy' 
nodes when any of the worker nodes is in failing state 


One of the method of configuring hashtag#HighAvailability on hashtag#Kubernetes is configuring etcd (distributed db) under two modes:
👉 𝐒𝐭𝐚𝐜𝐤𝐞𝐝 𝐞𝐭𝐜𝐝 𝐭𝐨𝐩𝐨𝐥𝐨𝐠𝐲 - This is the default mode of bring-up of distributed database provided by etcd that is brought upon the control plane nodes. 
This topology couples the control planes and etcd members on the same nodes. 
📌 Pros 
 ✅ Simpler to set up than a cluster with external etcd nodes
 ✅ Easier for managing replication.
 ✅ Reduces potential latency and network-related issues as communication is internal
 🛑 Cons
 ❗ Runs the risk of failed coupling, redundancy is compromised
 ❗Co-locating etcd with other Kubernetes components may lead to resource contention, especially in environments with limited resources
 ❗Limited Scalability due to by-design nature

👉 𝐄𝐱𝐭𝐞𝐫𝐧𝐚𝐥 𝐞𝐭𝐜𝐝 𝐭𝐨𝐩𝐨𝐥𝐨𝐠𝐲 - Here etcd resides on a separate nodes and each etcd host communicates with the kube-apiserver of each control plane node
📌 Pros 
 ✅ Decouples the control plane and etcd member, losing a control plane instance or an etcd member has less impact and does not affect the cluster redundancy 
 ✅ With an external etcd cluster, one have more control over its scalability independent of hashtag#k8s cluster, offering more flexibility 
 ✅ External etcd clusters can potentially offer better performance, especially in scenarios where the etcd workload is heavy or requires significant resources
 🛑 Cons
 ❗ Managing an external etcd cluster adds complexity to the Kubernetes infrastructure
 ❗Network issues or latency as it's external to the cluster 
 ❗Security Concerns as exposing etcd to external networks introduces potential security risk

𝑲𝒖𝒃𝒆𝒍𝒆𝒕, 𝑪𝒐𝒏𝒕𝒂𝒊𝒏𝒆𝒓 𝑹𝒖𝒏𝒕𝒊𝒎𝒆 𝒂𝒏𝒅 𝑪𝑵𝑰 𝑷𝒍𝒖𝒈𝒊𝒏𝒔 - 𝒉𝒐𝒘 𝒕𝒉𝒆𝒚 𝒂𝒓𝒆 𝒂𝒍𝒍 𝒔𝒕𝒊𝒕𝒄𝒉𝒆𝒅 𝒕𝒐𝒈𝒆𝒕𝒉𝒆𝒓 🤔 ☸ 

Cri plugin uses hashtag#containerd to manage the full hashtag#container lifecycle and all container images while managing hashtag#pod networking via hashtag#CNI

Let's understand what happens behind a pod creation :-
👉 Kubelet calls the cri plugin, via the CRI runtime service API, to create a pod
👉 CRI uses containerd internal to create and start a special pause container (the sandbox container) and put that container inside the pod’s cgroups and namespace 
 
