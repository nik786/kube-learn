
 A new service deployed in Kubernetes cannot communicate with a backend database. 
 What steps would you take to diagnose the issue?

 | **Step** | **Description** |
|---------|---------------|
| **Check Pod Status** | Run `kubectl get pods -n <namespace>` to ensure the service pod is running and not in `CrashLoopBackOff` or `Pending` state. |
| **Inspect Pod Logs** | Use `kubectl logs <pod-name> -n <namespace>` to check for connection errors in the application logs. |
| **Verify Service & Endpoints** | Run `kubectl get svc -n <namespace>` and `kubectl get endpoints -n <namespace>` to ensure the database service has valid endpoints. |
| **Test Internal DNS Resolution** | Use `nslookup <db-service-name>.<namespace>.svc.cluster.local` inside the pod to verify DNS resolution. |
| **Check Network Policies** | Run `kubectl get networkpolicy -n <namespace>` to check if any NetworkPolicies are blocking traffic. |
| **Verify Database Connectivity from Pod** | Use `kubectl exec -it <pod-name> -- nc -vz <db-host> <db-port>` to test network access from the pod to the database. |
| **Inspect Firewall & Security Groups** | Ensure cloud firewall rules (AWS Security Groups, GCP VPC Firewall) allow database connections from the cluster nodes. |
| **Check Kubernetes Ingress/Egress Rules** | If using a private database, confirm that the cluster nodes or VPC have access to the database endpoint. |
| **Validate Database Authentication** | Ensure correct credentials, secrets, and ConfigMaps are being used by running `kubectl get secrets -n <namespace>`. |
| **Confirm Database Readiness & Health** | Check database logs for errors and confirm it is accepting connections using `SHOW PROCESSLIST;` (MySQL) or `pg_stat_activity` (PostgreSQL). |
| **Ensure Correct Database Host & Port** | Verify that the database hostname, port, and protocol are correctly set in the application config (`kubectl describe configmap`). |
| **Check Service Account & RBAC Permissions** | If using cloud-managed databases, ensure the Kubernetes service account has necessary IAM permissions. |
| **Monitor Kubernetes Events** | Run `kubectl get events -n <namespace>` to check for any pod scheduling or networking issues. |
| **Test External Connectivity to Database** | If possible, connect from a non-Kubernetes environment (e.g., local machine or bastion host) to verify if the database is reachable. |
| **Restart Service Pods** | If configuration updates were made, restart pods with `kubectl rollout restart deployment <deployment-name> -n <namespace>`. |



You notice high latency when accessing a cloud-based application. 
What factors could be contributing, and how would you mitigate them


| **Factor** | **Possible Causes** | **Mitigation Strategies** |
|-----------|------------------|------------------------|
| **Network Latency** | Long geographical distance between users and servers. | Deploy CDN (CloudFront, Cloudflare) and use multi-region deployments. |
| **High API Response Time** | Backend services taking too long to process requests. | Optimize database queries, use caching (Redis, Memcached), and enable asynchronous processing. |
| **DNS Resolution Delay** | Slow DNS lookups or incorrect DNS configurations. | Use low TTL values and DNS providers with fast resolution (e.g., AWS Route 53, Google DNS). |
| **Database Bottlenecks** | Slow queries, unoptimized indexes, or high connection load. | Optimize indexes, use read replicas, and implement connection pooling. |
| **Resource Contention** | CPU, memory, or disk I/O saturation on application servers. | Autoscale instances, optimize instance sizing, and enable vertical/horizontal scaling. |
| **Poor Load Balancing** | Traffic not distributed efficiently across instances. | Use load balancers (AWS ALB/ELB, NGINX, HAProxy) and enable health checks. |
| **Excessive HTTP Requests** | Too many HTTP requests causing congestion. | Enable HTTP/2, reduce unnecessary API calls, and batch requests where possible. |
| **Inefficient Frontend Performance** | Large assets, unoptimized JavaScript, and high render time. | Minify JS/CSS, use lazy loading, and optimize images using WebP format. |
| **Container Orchestration Issues** | Slow pod scheduling, resource limits in Kubernetes. | Tune Kubernetes resource requests/limits, and enable Cluster Autoscaler. |
| **Serverless Cold Starts** | Delay in spinning up serverless functions (Lambda, Cloud Functions). | Use provisioned concurrency, keep functions warm with scheduled invocations. |
| **TLS/SSL Overhead** | Expensive encryption/decryption processes. | Use TLS session resumption and enable HTTP/2 to reduce handshake overhead. |
| **Cloud Provider Throttling** | Hitting API rate limits or cloud service quotas. | Upgrade service tiers, increase rate limits, and implement exponential backoff for retries. |
| **Packet Loss & Network Jitter** | Poor internet connectivity between users and cloud. | Implement TCP optimizations, use Anycast routing, and enable QoS settings. |
| **Application Code Inefficiencies** | Poorly written code causing delays. | Profile code performance using APM tools (New Relic, Datadog) and optimize bottlenecks. |



Your team reports frequent SSH connection timeouts when accessing production servers. How would you debug this issue?

| **Step** | **Description** |
|---------|---------------|
| **Check Server Availability** | Run `ping <server-ip>` to see if the server is reachable. |
| **Verify SSH Service Status** | Use `systemctl status sshd` or `service ssh status` to check if the SSH daemon is running. |
| **Inspect SSH Logs** | Check logs (`/var/log/auth.log` or `/var/log/secure`) for errors related to SSH failures. |
| **Test Network Connectivity** | Use `telnet <server-ip> 22` or `nc -vz <server-ip> 22` to verify if the SSH port is accessible. |
| **Check Firewall Rules** | Run `iptables -L -n` or `firewalld` rules to ensure port 22 is open. |
| **Verify Cloud Security Group Rules** | If using AWS, GCP, or Azure, check that security groups allow inbound SSH connections. |
| **Analyze Load & Resource Usage** | Use `top`, `htop`, or `dmesg` to check for CPU/memory exhaustion that could impact SSH responsiveness. |
| **Identify Rate Limiting or Fail2Ban Blocks** | Check if `fail2ban` or `DenyHosts` is blocking IPs due to failed login attempts (`fail2ban-client status sshd`). |
| **Test from Another Network** | Try SSH from a different network (e.g., mobile hotspot) to rule out ISP or VPN issues. |
| **Verify SSH Client Configurations** | Ensure proper settings in `~/.ssh/config` and check `ssh -vvv <server-ip>` for debugging details. |
| **Restart SSH Service** | Restart the SSH daemon using `systemctl restart sshd` and try reconnecting. |
| **Check TCP Keepalive Settings** | Modify `ClientAliveInterval` and `ClientAliveCountMax` in `/etc/ssh/sshd_config` to keep connections alive. |
| **Review Network Latency & Packet Loss** | Run `mtr <server-ip>` or `traceroute <server-ip>` to detect network issues. |
| **Ensure Sufficient Open File Descriptors** | Run `ulimit -n` to check if the system is hitting file descriptor limits, affecting SSH. |
| **Investigate VPN or Proxy Issues** | If accessing through VPN, check VPN logs and try reconnecting with a direct connection. |
| **Check Host-Based Authentication** | If using key-based authentication, ensure correct permissions (`chmod 600 ~/.ssh/authorized_keys`). |
| **Restart the Server (Last Resort)** | If all else fails, reboot the server and verify SSH access post-restart. |


Your CI/CD pipeline fails due to network timeouts while pulling Docker images. What could be the cause, and how do you fix it?


| **Possible Cause** | **Description** | **Solution** |
|-------------------|---------------|------------|
| **Network Connectivity Issues** | The CI/CD runner may have unstable internet connectivity. | Run `ping registry.hub.docker.com` or `curl -v https://registry.hub.docker.com` to check connectivity. Restart network services if needed. |
| **Docker Hub Rate Limits** | Docker Hub imposes pull rate limits for unauthenticated users. | Authenticate using `docker login` and use an image cache (e.g., local registry or AWS ECR/GCR/ACR). |
| **DNS Resolution Problems** | The runner may be unable to resolve the container registry hostname. | Use `nslookup registry.hub.docker.com` or `dig registry.hub.docker.com` and update `/etc/resolv.conf` or use Google's DNS (`8.8.8.8`). |
| **Proxy or Firewall Blocking Requests** | Corporate network policies may restrict Docker image pulls. | Configure proxy settings in Docker daemon (`/etc/docker/daemon.json`) and allow required domains. |
| **Overloaded CI/CD Runner** | High CPU, memory, or disk usage on the runner can cause timeouts. | Check resource usage with `top` or `htop`, and increase system resources if needed. |
| **Slow or Unstable Docker Registry** | The container registry (Docker Hub, AWS ECR, GCR, ACR) may be experiencing slow response times. | Switch to a different mirror, e.g., `https://mirror.gcr.io`, or use a self-hosted registry. |
| **Throttling by Cloud Provider** | Cloud service providers may throttle bandwidth during high usage. | Optimize concurrent jobs in the CI/CD pipeline and use image caching. |
| **Large Image Size** | Pulling very large images increases the risk of timeouts. | Use multi-stage builds, optimize images with smaller base images, and remove unnecessary dependencies. |
| **Outdated Docker Version** | Older Docker versions may have performance issues with registry communication. | Upgrade Docker using `apt update && apt install docker-ce -y` (Ubuntu) or `yum update docker-ce` (RHEL). |
| **Incorrect Docker Daemon Configuration** | Misconfigured Docker daemon settings can cause network issues. | Check `/etc/docker/daemon.json` for any incorrect proxy or MTU settings. |
| **CI/CD Pipeline Configuration Issues** | The pipeline may be pulling images too frequently or unnecessarily. | Use `docker pull --quiet`, leverage image caching, and avoid redundant pulls. |
| **Registry Authentication Issues** | Private container registries may require authentication. | Ensure valid credentials are stored in `~/.docker/config.json` or CI/CD secrets management. |
| **Cloud Outage or Registry Downtime** | The container registry service may be temporarily unavailable. | Check registry status (`https://status.docker.com/` for Docker Hub) and retry later. |




| **Possible Cause** | **Description** | **Solution** |
|-------------------|---------------|------------|
| **High CPU or Memory Utilization** | Excessive CPU or RAM usage can slow down performance. | Check with `top` or `htop`, and optimize processes. Consider upgrading the instance type. |
| **Network Congestion** | High traffic on the network can cause delays. | Use `iftop` or `netstat` to check network usage. Consider switching to an instance with better network performance (e.g., ENA-enabled). |
| **Overloaded Disk I/O** | High read/write operations on disk can cause latency. | Monitor with `iostat` or `iotop`. Use SSD-backed EBS volumes (`gp3` or `io2`) for better performance. |
| **Improper Instance Type** | Some instance types may not be suitable for high-performance workloads. | Choose an instance optimized for your workload (e.g., compute-optimized `C` series or memory-optimized `R` series). |
| **AWS Placement Group Misconfiguration** | Without a placement group, inter-instance latency might be high. | Use placement groups (`cluster` type) for low-latency networking. |
| **Inefficient Security Group Rules** | Overly restrictive security group rules can delay packet processing. | Optimize security groups and use AWS VPC Flow Logs to diagnose issues. |
| **Outdated Kernel or OS** | An old kernel or OS can introduce performance bottlenecks. | Update OS and kernel (`sudo yum update -y` or `sudo apt update && sudo apt upgrade -y`). |
| **Bad Network MTU Settings** | Incorrect MTU size can cause packet fragmentation and retransmission. | Set MTU to `9001` for Jumbo Frames on enhanced networking (`ip link set eth0 mtu 9001`). |
| **No Enhanced Networking** | Standard networking drivers may not be optimized for high-performance workloads. | Enable Enhanced Networking (`ENA` or `SR-IOV`) for better throughput. |
| **AWS Throttling or No Burst Credits** | EC2 instances (especially `t2/t3`) may experience CPU throttling. | Check `aws cloudwatch get-metric-data` for CPU credits and upgrade to `t3.unlimited` or `m5` series if needed. |
| **High Latency to External Services** | Calls to external APIs or databases can cause slowness. | Optimize DNS resolution, use VPC Endpoints, and enable caching where possible. |
| **Improper Load Balancing** | Unoptimized traffic routing can lead to high latency. | Use an Elastic Load Balancer (ELB) or Route 53 latency-based routing. |
| **DNS Resolution Delay** | Slow DNS lookups can impact application performance. | Use AWS Route 53 or Google’s `8.8.8.8` for faster lookups and enable DNS caching. |
| **Unoptimized Software Configuration** | Misconfigured applications or services can cause delays. | Profile application performance using `strace`, `perf`, or APM tools like AWS X-Ray. |








