Installation of Specific Versions of Kubeadm
https://www.thetopsites.net/article/50447714.shtml
https://platform9.com/blog/kubernetes-upgrade-the-definitive-guide-to-do-it-yourself/
https://medium.com/@vivek_syngh/setup-a-single-node-kubernetes-cluster-on-ubuntu-16-04-6412373d837a
https://www.linuxtechi.com/install-configure-kubernetes-ubuntu-18-04-ubuntu-18-10/
https://www.gremlin.com/community/tutorials/how-to-create-a-kubernetes-cluster-on-ubuntu-16-04-with-kubeadm-and-weave-net/
https://github.com/kubernetes/kubeadm/issues/1438
https://docs.genesys.com/Documentation/GCXI/9.0.0/Dep/DockerOffline
https://forum.linuxfoundation.org/discussion/846483/lab2-1-kubectl-untainted-not-working
https://stackoverflow.com/questions/43147941/allow-scheduling-of-pods-on-kubernetes-master
https://github.com/projectcalico/cni-plugin/releases/tag/v3.0.10


Installation of kubeadm

1. Install and configure prereqisties

apt-get install docker.io -y
apt-get install apt-transport-https curl -y
swapoff -a
pip install netaddr

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \
  echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list && \
  sudo apt-get update -q


2. Install Kubeadm 1.17.3-00

apt-get install  kubeadm=1.17.3-00 kubelet=1.17.3-00 kubectl=1.17.3-00

3. Configure docker service
vim /lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd


4. Stop start docker service

systemctl daemon-reload
systemctl stop docker
systemctl start docker
systemctl enable docker

5. Enable bridge-nf-call-iptables
vim /etc/sysctl.conf

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
sysctl -p
swapoff -a



6. Pull kube proxy container

docker pull k8s.gcr.io/kube-proxy:v1.17.3
docker pull k8s.gcr.io/kube-proxy:v1.19.3

7. Initialise kubeadm with network cidr
   kubeadm init --apiserver-advertise-address=192.168.56.167 --pod-network-cidr=192.168.56.0/24
   
8.   Join the node 
  kubeadm join 192.168.56.167:6443 --token ldi5zt.itm2rbj3xncg3bp1     --discovery-token-ca-cert-hash sha256:ff6012b6d052d1807396001ea1517d36f7c5c7852012fa6a60b15bf23a6dbb38 --ignore-preflight-errors='DirAvailable--etc-kubernetes-manifests,FileAvailable--etc-kubernetes-kubelet.conf,Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt'
 
9. Taint the master node
kubectl taint nodes --all node-role.kubernetes.io/master-
kubectl describe node k9i | grep -i Taints
kubectl taint node k9i node.kubernetes.io/not-ready:NoSchedule-

10. Apply calico 
kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml
  
2. Deploy Nginx deployment controller

3. Take backup of etcd

4. Upgrade kueadm 1.17.3-00 to kubeadm 1.18.00


5. Restore etcd


 kubectl taint nodes --all node-role.kubernetes.io/master-

kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>


vim /var/lib/kubelet/kubeadm-flags.env


KUBELET_KUBEADM_ARGS="--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --exec-opt native.cgroupdriver=systemd"
KUBELET_KUBEADM_ARGS="--cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1 --resolv-conf=/run/systemd/resolve/resolv.conf --cluster-domain=cluster.local"
KUBELET_KUBEADM_ARGS="--pod-infra-container-image=k8s.gcr.io/pause:3.1"


  














kubectl describe node k9i | grep -i Taints
kubectl taint node k9i node.kubernetes.io/not-ready:NoSchedule-
kubectl taint node k9i node-role.kubernetes.io/master-





Upgrade

apt-get install kubelet=1.18.0-00 kubectl=1.18.0-00
kubeadm upgrade apply v1.18.0
systemctl restart kubelet



Backup Process

Backup Command

export ETCDCTL_API=3

./etcdctl --endpoints=https://192.168.56.167:2379 
--cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key snapshot save etcd-snapshot.db



Backup Status
./etcdctl --endpoints=https://192.168.56.167:2379 
--cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key snapshot status /opt/backup/etcd-snapshot.db


Restore Command
export ETCDCTL_API=3
./etcdctl --endpoints="https://192.168.56.167:2379" 
--name=k9i --cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key 
--initial-cluster="k9i=https://192.168.56.167:2380" 
--initial-advertise-peer-urls="https://192.168.56.167:2380" 
--data-dir="/var/lib/etcd-restored" 
--initial-cluster-token="etcd-cluster" snapshot restore /opt/backup/etcd-snapshot.db


drain : pods are gracefully terminated from node on which they are on and recreated on another

cordon : unshedulable
no pods can be scheduled on this node untill remove restriction

