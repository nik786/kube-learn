Installation of Specific Versions of Kubeadm
https://www.thetopsites.net/article/50447714.shtml
https://platform9.com/blog/kubernetes-upgrade-the-definitive-guide-to-do-it-yourself/
https://medium.com/@vivek_syngh/setup-a-single-node-kubernetes-cluster-on-ubuntu-16-04-6412373d837a
https://www.linuxtechi.com/install-configure-kubernetes-ubuntu-18-04-ubuntu-18-10/
https://www.gremlin.com/community/tutorials/how-to-create-a-kubernetes-cluster-on-ubuntu-16-04-with-kubeadm-and-weave-net/
https://github.com/kubernetes/kubeadm/issues/1438
https://docs.genesys.com/Documentation/GCXI/9.0.0/Dep/DockerOffline
https://forum.linuxfoundation.org/discussion/846483/lab2-1-kubectl-untainted-not-working
https://stackoverflow.com/questions/43147941/allow-scheduling-of-pods-on-kubernetes-master
https://github.com/projectcalico/cni-plugin/releases/tag/v3.0.10


Installation of kubeadm

1. Install and configure prereqisties

apt-get install docker.io -y
apt-get install apt-transport-https curl -y
swapoff -a
pip install netaddr

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \
  echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list && \
  sudo apt-get update -q


2. Install Kubeadm 1.17.3-00

apt-get install  kubeadm=1.17.3-00 kubelet=1.17.3-00 kubectl=1.17.3-00
apt-get install  kubeadm=1.18.0-00 kubelet=1.18.0-00 kubectl=1.18.0-00

3. Configure docker service
vim /lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd


4. Stop start docker service

systemctl daemon-reload
systemctl stop docker
systemctl start docker
systemctl enable docker

5. Enable bridge-nf-call-iptables
vim /etc/sysctl.conf

net.bridge.bridge-nf-call-ip6tables = 1
#net.bridge.bridge-nf-call-iptables = 1
#net.ipv4.ip_forward = 1
sysctl -p
swapoff -a


Point number 6 is not required
6. Pull kube proxy container

docker pull k8s.gcr.io/kube-proxy:v1.17.3
docker pull k8s.gcr.io/kube-proxy:v1.19.3

7. Initialise kubeadm with network cidr
   kubeadm init --apiserver-advertise-address=192.168.56.167 --pod-network-cidr=192.168.56.0/24
   kubeadm init --apiserver-advertise-address=192.168.56.72 --pod-network-cidr=192.168.56.0/24
   
   
8.  First deploy kube-flannel to bring cluster on ready state
   
   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
   
   
9.  then execute below mentioned command to schedule pods on master nodes
   
   kubectl taint nodes --all node-role.kubernetes.io/master-
   kubectl describe node k9i | grep -i Taints
   
   
   
10. Then get the latest calico.yml file and verify the image version in calico.yml( Deployment of calico network controller)
   
   curl https://docs.projectcalico.org/manifests/calico.yaml -O
   

   kubectl apply -f calico.yml
   
   Below mentioned calico.yml is outdated . It can be updated . But need to consutruct below mnetioned command with latest version.
   
   
   kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml
   
   wget https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml

kubectl apply -f 
https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml
kubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

   
   
   
   
   
   
   
 11. Delete flannel network policy  

   
12. Join the node 
   kubeadm join 192.168.56.167:6443 --token ldi5zt.itm2rbj3xncg3bp1     --discovery-token-ca-cert-hash sha256:ff6012b6d052d1807396001ea1517d36f7c5c7852012fa6a60b15bf23a6dbb38 --ignore-preflight-errors='DirAvailable--etc-kubernetes-manifests,FileAvailable--etc-kubernetes-kubelet.conf,Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt'
 
12. number point is nt required for single node cluster



Testing of created cluster

1. Deploy Nginx pod 
2. Deploy tomcat  pod
3. Execute dns mapping command from tomcat container
4. Ping nginx container from tomcat pod
5. access nginx endpoint from tomcat container


kubectl run nginx --image=nginx --replicas=1

kubectl create deployment nginx --image=nginx
kubectl scale deployment nginx --replicas=1 -n np
kubectl expose deployment nginx --type=NodePort --port 80 --target-port 80



kubectl run tomcat --image=tomcat:8.0 --replicas=1  
kubectl exec -ti  tomcat -- bash
kubectl exec -ti  nginx -- bash

kubectl exec -ti  nginx -- cat /etc/resolv.conf
kubectl exec -ti  nginx -- nslookup kubernetes.default.svc.cluster.local



cat /etc/resolv.conf

nslookup kubernetes.default.svc.cluster.local 10.254.0.20
nslookup kubernetes.default.svc.cluster.local

curl -I http://10.1.121.10

kubectl exec tomcat-9f85b9559-8qs4f -- curl -s -k -I  http://10.1.121.17








kubectl expose deployment simple-webapp-deployment --name=webapp-service --port=8080 
--target-port=8080 --type=NodePort --selector=name=simple-webapp --overrides 
'{ "apiVersion": "v1","spec":{"ports":[{"port": 8080,"protocol":"TCP","targetPort": 8080,"nodePort": 30380}]}}'

kubectl create service nodeport webapp-service  --node-port=31000 --tcp=80:80


Cat /etc/resolv.conf

nslookup kubernetes.default.svc.cluster.local 10.254.0.20
nslookup kubernetes.default.svc.cluster.local
nslookup nginx.np.svc.cluster.local
nslookup nginx.default.svc.cluster.local

curl -I http://10.1.121.10

while true; do curl -I http://hpa-nginx/; done

for i in {1..10};do ab -n 100 -c 2 https://10.233.75.226;done

while true; do ab -n 1000 -c 2 http://hpa-nginx/; done


nslookup kubernetes.default.svc.cluster.local 10.254.0.20
nslookup kubernetes.default.svc.cluster.local
kubectl exec -ti dnsutils -- nslookup kubernetes.default
kubectl exec -ti hostnames-59cc46cdcf-42dbt -- cat /etc/resolve.conf
kubectl exec -ti dnsutils -- cat /etc/resolve.conf

apt-get update  
apt-get install dnsutils net-tools telnet














1. Take backup of etcd

2. Upgrade kueadm 1.17.3-00 to kubeadm 1.18.00


3. Restore etcd


Debug Steps of K8s cluster

kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>


vim /var/lib/kubelet/kubeadm-flags.env


KUBELET_KUBEADM_ARGS="--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --exec-opt native.cgroupdriver=systemd"
KUBELET_KUBEADM_ARGS="--cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1 --resolv-conf=/run/systemd/resolve/resolv.conf --cluster-domain=cluster.local"
KUBELET_KUBEADM_ARGS="--pod-infra-container-image=k8s.gcr.io/pause:3.1"


  


Upgrade

apt-get install kubelet=1.18.0-00 kubectl=1.18.0-00
kubeadm upgrade apply v1.18.0
systemctl restart kubelet



Backup Process

Backup Command

export ETCDCTL_API=3

./etcdctl --endpoints=https://192.168.56.167:2379 
--cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key snapshot save etcd-snapshot.db



Backup Status
./etcdctl --endpoints=https://192.168.56.167:2379 
--cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key snapshot status /opt/backup/etcd-snapshot.db


Restore Command
export ETCDCTL_API=3
./etcdctl --endpoints="https://192.168.56.167:2379" 
--name=k9i --cacert /etc/kubernetes/pki/etcd/ca.crt 
--cert /etc/kubernetes/pki/apiserver-etcd-client.crt 
--key /etc/kubernetes/pki/apiserver-etcd-client.key 
--initial-cluster="k9i=https://192.168.56.167:2380" 
--initial-advertise-peer-urls="https://192.168.56.167:2380" 
--data-dir="/var/lib/etcd-restored" 
--initial-cluster-token="etcd-cluster" snapshot restore /opt/backup/etcd-snapshot.db


drain : pods are gracefully terminated from node on which they are on and recreated on another

cordon : unshedulable
no pods can be scheduled on this node untill remove restriction

